{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":"<p>En la era digital actual, el campo de la Ingenier\u00eda en Telecomunicaciones se encuentra en constante evoluci\u00f3n, impulsado por la creciente demanda de conectividad y el acceso a datos a una escala sin precedentes. Dentro de este panorama, el Machine Learning ha surgido como una herramienta poderosa para extraer conocimientos valiosos de estos datos, transformando la manera en que interactuamos con la informaci\u00f3n y mejorando la toma de decisiones en una amplia gama de aplicaciones. En este contexto, las pr\u00e1cticas profesionales desempe\u00f1an un papel crucial al abordar desaf\u00edos complejos y relevantes que enfrenta la industria de las telecomunicaciones.</p>"},{"location":"#objetivos-generales","title":"Objetivos Generales","text":"<ul> <li> <p>Explorar y evaluar un conjunto de tecnolog\u00edas esenciales que permiten la implementaci\u00f3n exitosa de pr\u00e1cticas de Machine Learning Operations (MLOps) en el entorno universitario, haciendo hincapi\u00e9 en la reutilizaci\u00f3n, la reproducibilidad y la escalabilidad. La implementaci\u00f3n de sistemas de MLOps se ha vuelto cada vez m\u00e1s crucial, ya que proporciona la infraestructura necesaria para respaldar a los cient\u00edficos y expertos en la universidad en su b\u00fasqueda de soluciones basadas en Machine Learning. Facilita la gesti\u00f3n eficiente del ciclo de vida de los modelos, desde su desarrollo y entrenamiento hasta su despliegue y monitorizaci\u00f3n, permitiendo as\u00ed la reutilizaci\u00f3n de los recursos, la reproducibilidad de los resultados y la escalabilidad de las soluciones.</p> </li> <li> <p>El an\u00e1lisis y comparaci\u00f3n de las siguientes tecnolog\u00edas: Airflow, Kubeflow, MLFlow y DVC. Cada una de estas herramientas tiene su conjunto \u00fanico de caracter\u00edsticas y capacidades que las hacen adecuadas para diferentes escenarios y requisitos. La selecci\u00f3n de la soluci\u00f3n m\u00e1s id\u00f3nea para el Cluster de la Universidad ser\u00e1 el resultado de una evaluaci\u00f3n exhaustiva, teniendo en cuenta aspectos como la reutilizaci\u00f3n de los componentes, la reproducibilidad de los experimentos y la escalabilidad de las soluciones implementadas, permitiendo la facilidad de uso y la capacidad de gestionar eficazmente los flujos de trabajo de Machine Learning.</p> </li> <li> <p>La implementaci\u00f3n exitosa de la tecnolog\u00eda seleccionada en el Cluster de la Universidad, demostrando as\u00ed el compromiso de la instituci\u00f3n con la adopci\u00f3n de las mejores pr\u00e1cticas para los desaf\u00edos tecnol\u00f3gicos del futuro.</p> </li> </ul>"},{"location":"#objetivos-especificos","title":"Objetivos espec\u00edficos","text":"<ul> <li>Investigar y evaluar tecnolog\u00edas clave en el campo de MLOps, centr\u00e1ndose en la reutilizaci\u00f3n, reproducibilidad y escalabilidad.</li> <li>Seleccionar la herramienta m\u00e1s adecuada, entre Airflow, Kubeflow, MLFlow y DVC, para implementar pr\u00e1cticas eficientes de MLOps en el entorno universitario.</li> <li>Dise\u00f1ar e implementar una infraestructura basada en la herramienta seleccionada, que permita gestionar el ciclo de vida completo de los modelos de Machine Learning.</li> <li>Desarrollar pipelines de flujo de trabajo utilizando la herramienta seleccionada, para automatizar tareas como el entrenamiento, evaluaci\u00f3n y despliegue de modelos.</li> <li>Integrar modelos de Machine Learning en los pipelines, asegurando la correcta reproducci\u00f3n de los resultados y la reutilizaci\u00f3n de componentes.</li> <li>Realizar pruebas exhaustivas y ajustes en los pipelines implementados, garantizando su funcionamiento correcto y eficiente.</li> <li>Redactar un informe final que documente el proceso de implementaci\u00f3n, destacando la reutilizaci\u00f3n, reproducibilidad y escalabilidad logradas, as\u00ed como los beneficios obtenidos para la universidad en el \u00e1mbito de MLOps.</li> </ul>"},{"location":"2_workplan/","title":"Plan de Trabajo","text":"<p>Las actividades deben detallar claramente c\u00f3mo se alcanzar\u00e1 cada uno de los objetivos planteados, indicando brevemente o enunciando la metodolog\u00eda de trabajo, t\u00e9cnica anal\u00edtica, entre otros. La actividad 1 debe contener el an\u00e1lisis de riesgos asociados propios al \u00e1rea, l\u00ednea de trabajo.</p>"},{"location":"2_workplan/#cronograma-de-actividades","title":"Cronograma de actividades","text":"<p>Del 27 de Septiembre al 7 de Diciembre de 2023 con 6 horas diarias en la franja horaria de 8 a 14 hs.</p> <ul> <li> <p>Actividad N\u00ba1 (A1): B\u00fasqueda y revisi\u00f3n bibliogr\u00e1fica. Contextualizaci\u00f3n. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba2 (A2):  Investigaci\u00f3n y evaluaci\u00f3n de la herramienta. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba3 (A3): Dise\u00f1o e implementaci\u00f3n de la infraestructura basada en Kubeflow . Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba4 (A4): Desarrollo de pipelines de flujo de trabajo. Duraci\u00f3n: 48 horas.</p> </li> <li> <p>Actividad N\u00ba5 (A5): Integraci\u00f3n de modelos de Machine Learning en los pipelines. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba6 (A6): Pruebas y ajustes de los pipelines. Duraci\u00f3n: 36 horas.</p> </li> <li> <p>Actividad N\u00ba7 (A7): Redacci\u00f3n de documentaci\u00f3n e informe. Duraci\u00f3n: 58 horas.</p> </li> </ul>"},{"location":"2_workplan/#septiembre-2023","title":"Septiembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 35 28 29 30 31 1 2 3 36 4 5 6 7 8 9 10 37 11 12 13 14 15 16 17 38 18 19 20 21 22 23 24 39 25 26 27 (A1) 28 (A1) 29 (A1) 30 1"},{"location":"2_workplan/#octubre-2023","title":"Octubre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 39 25 26 27 28 29 30 1 40 2  (A1) 3 (A1) 4 (A1) 5 (A1) 6 (A2) 7 8 41 9 (A2) 10 (A2) 11 (A2) 12 (A2) 13 (A2) 14 15 42 16 (A2) 17 (A3) 18 (A3) 19 (A3) 20 (A3) 21 22 43 23 (A3) 24 (A3) 25 (A3) 26 (A4) 27 (A4) 28 29 44 30 (A4) 31 (A4) 1 2 3 4 5"},{"location":"2_workplan/#noviembre-2023","title":"Noviembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 44 30 31 1 (A4) 2 (A4) 3 (A4) 4 5 45 6 (A4) 7 (A5) 8 (A5) 9 (A5) 10 (A5) 11 12 46 13 (A5) 14 (A5) 15 (A5) 16 (A5) 17 (A6) 18 19 47 20 (A6) 21 (A6) 22 (A6) 23 (A6) 24 (A6) 25 26 48 27 (A7) 28 (A7) 29 (A7) 30 (A7) 1 2 3"},{"location":"2_workplan/#diciembre-2023","title":"Diciembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 48 27 28 29 30 1(A7) 2 3 49 4(A7) 5(A7) 6(A7) 7(A7) 8 9 10 50 11 12 13 14 15 16 17 51 18 19 20 21 22 23 24 52 25 26 27 28 29 30 31"},{"location":"3_body/","title":"Desarrollo","text":""},{"location":"3_body/#comparacion","title":"Comparaci\u00f3n","text":"<p>Kubeflow ofrece una forma escalable de entrenar y desplegar modelos en Kubernetes. Es un medio de orquestaci\u00f3n que permite que un framework de aplicaciones en la nube funcione sin problemas. Algunos de los componentes de Kubeflow son los siguientes:</p> <ul> <li> <p>Notebooks: Ofrece servicios para crear y gestionar cuadernos Jupyter interactivos en entornos corporativos. Tambi\u00e9n incluye la posibilidad de que los usuarios construyan contenedores de Notebooks o pods directamente en clusters.</p> </li> <li> <p>Entrenamiento de modelos de TensorFlow: Kubeflow viene con un \"job operator\" de TensorFlow personalizado que facilita la configuraci\u00f3n y ejecuci\u00f3n del entrenamiento de modelos en Kubernetes. Kubeflow tambi\u00e9n admite otros frameworks mediante job operators a medida, pero su madurez puede variar.</p> </li> <li> <p>Pipelines: Los pipelines de Kubeflow permiten construir y gestionar flujos de trabajo de aprendizaje autom\u00e1tico de m\u00faltiples pasos que se ejecutan en contenedores Docker.</p> </li> <li> <p>Despliegue: Kubeflow ofrece varias formas de desplegar modelos en Kubernetes a trav\u00e9s de complementos externos.</p> </li> </ul> <p>MLflow es un framework de c\u00f3digo abierto para el seguimiento de todo el ciclo de aprendizaje autom\u00e1tico de principio a fin, desde la formaci\u00f3n hasta la implementaci\u00f3n. Entre las funciones que ofrece se encuentran el seguimiento de modelos, la gesti\u00f3n, el empaquetado y las transiciones centralizadas de etapas del ciclo de vida. Algunos de los componentes de MLflow son los siguientes:</p> <ul> <li> <p>Seguimiento: Mientras ejecutas tu c\u00f3digo de aprendizaje autom\u00e1tico, hay una API y una interfaz de usuario para registrar par\u00e1metros, versiones de c\u00f3digo, m\u00e9tricas y archivos de salida para que puedas visualizarlos m\u00e1s tarde.</p> </li> <li> <p>Proyecto: Proporcionan un estilo est\u00e1ndar para empaquetar c\u00f3digo de ciencia de datos reutilizable; no obstante, cada proyecto es un directorio de c\u00f3digo o un repositorio Git que utiliza un archivo descriptor para indicar las dependencias y c\u00f3mo ejecutar el c\u00f3digo.</p> </li> <li> <p>Modelos: Los modelos MLflow son un est\u00e1ndar para la distribuci\u00f3n de modelos de aprendizaje autom\u00e1tico en una variedad de sabores. Hay varias herramientas disponibles para ayudar con el despliegue de varios modelos. Cada modelo se guarda como un directorio con archivos arbitrarios y un archivo de descripci\u00f3n del modelo ML que identifica los sabores en los que se puede utilizar.</p> </li> <li> <p>Registro: Le ofrece un almac\u00e9n de modelos centralizado, una interfaz de usuario y un conjunto de API para gestionar de forma colaborativa el ciclo de vida completo de su modelo MLflow. Proporciona linaje de modelos, versiones de modelos, transiciones de etapas y anotaciones.</p> </li> </ul> <p>Airflow es una plataforma de gesti\u00f3n de flujos de trabajo de c\u00f3digo abierto creada por Airbnb en 2014 para crear, supervisar y programar mediante programaci\u00f3n los crecientes flujos de trabajo de la empresa. Algunos de los componentes de Airflow son los siguientes:</p> <ul> <li> <p>Scheduler: Supervisa las tareas y los DAG, activa los flujos de trabajo programados y env\u00eda las tareas al ejecutor para que las ejecute. Est\u00e1 dise\u00f1ado para ejecutarse como un servicio persistente en el entorno de producci\u00f3n de Airflow.</p> </li> <li> <p>Ejecutores: Son mecanismos que ejecutan instancias de tareas; pr\u00e1cticamente ejecutan todo en el planificador. Los ejecutores tienen una API com\u00fan y puede intercambiarlos en funci\u00f3n de los requisitos de su instalaci\u00f3n. S\u00f3lo puede tener configurado un ejecutor por vez.</p> </li> <li> <p>Servidor web: Una interfaz de usuario que muestra el estado de sus trabajos y le permite ver, activar y depurar DAGs (**) y tareas. Tambi\u00e9n le ayuda a interactuar con la base de datos y a leer registros del almac\u00e9n de archivos remoto.</p> </li> <li> <p>Base de datos de metadatos: La base de datos de metadatos es utilizada por el ejecutor, el servidor web y el scheduler para almacenar el estado.</p> </li> </ul> <p>Data Version Control(DVC) es un sistema de control de versiones de c\u00f3digo abierto utilizado en proyectos de aprendizaje autom\u00e1tico. Tambi\u00e9n se conoce como Git para ML. Se ocupa de las versiones de datos en lugar de las versiones de c\u00f3digo. DVC le ayuda a lidiar con grandes modelos y archivos de datos que no pueden ser manejados usando Git. Le permite almacenar informaci\u00f3n sobre las diferentes versiones de sus datos para realizar un seguimiento adecuado de los datos de ML y acceder al rendimiento de su modelo m\u00e1s tarde. Puede definir un repositorio remoto para enviar sus datos y modelos, lo que facilita la colaboraci\u00f3n entre los miembros del equipo.</p> <p>Para obtener el resultado deseado, los usuarios no tienen que recordar manualmente qu\u00e9 modelo de datos utiliza qu\u00e9 conjunto de datos y qu\u00e9 acciones se llevaron a cabo; de todo esto se encarga DVC. Consiste en un conjunto de herramientas y procesos que rastrean las versiones cambiantes de los datos y las colecciones de datos anteriores. Los repositorios de DVC contienen los archivos que est\u00e1n bajo el efecto del sistema de control de versiones. Se mantiene un estado clasificado para cada cambio que se confirma en cualquier archivo de datos.</p> <p></p> <p>Luego de analizar las capacidades de cada uno de los sistemas vistos anteriormente, podemos decir que si nuestro sistema necesita tratar con m\u00faltiples tipos de flujo de trabajo, no s\u00f3lo aprendizaje autom\u00e1tico, Airflow puede ayudarnos mejor. Es un marco de orquestaci\u00f3n de flujos de trabajo maduro, con soporte para muchos operadores, adem\u00e1s del aprendizaje autom\u00e1tico.</p> <p>Si deseamos un sistema con patrones predise\u00f1ados para el aprendizaje autom\u00e1tico y que funcione a gran escala en cl\u00fasteres Kubenetes, podemos considerar Kubeflow. Muchos componentes espec\u00edficos de ML en Kubeflow pueden ahorrarnos tiempo a comparaci\u00f3n de si los hacemos con Airflow.</p> <p>Si queremos desplegar MLOps en un sistema a peque\u00f1a escala (por ejemplo, una estaci\u00f3n de trabajo, o un port\u00e1til), nos conviene elegir Airflow + MLflow, ya que elimina la necesidad de configurar y ejecutar un sistema Kubenetes, y ahorrar m\u00e1s recursos para las tareas principales.</p> <p>Como DVC se dedica a una porci\u00f3n muy espec\u00edfica y similar a MLFlow, queda en la misma categor\u00eda que el p\u00e1rrafo anterior, no cumpliendo el ciclo completo y necesitando de la combinaci\u00f3n con otro sistema.</p> <p>En nuestro caso de aplicaci\u00f3n, como nuestro cluster ya tiene Kubernetes y consideramos que es m\u00e1s completo y abarca todo el ciclo de trabajo completo, la elecci\u00f3n ideal ser\u00eda Kubeflow, por lo que ahondaremos m\u00e1s en el mismo para poder realizar su implementaci\u00f3n.</p> <p>() Job operator:* Es un recurso personalizado de Kubernetes que permite correr tareas de entrenamiento de TensorFlow en dicha plataforma.</p> <p>() DAGs: Directed Acyclic Graph, es una forma de modelar las redes neuronales en forma de nodos interconectados por flechas.</p>"},{"location":"3_body/#descripcion-del-entorno-de-trabajo","title":"Descripci\u00f3n del entorno de trabajo","text":""},{"location":"3_body/#recursos-fisicos","title":"Recursos f\u00edsicos","text":"<p>Conjunto de 3 PC's con los siguientes recursos cada una:</p> <ul> <li>CPU's: Ryzen 9 XXXX</li> <li>Memoria RAM: 32GB</li> <li>Almacenamiento: X GB</li> <li>GPU's: NO, por el momento.</li> </ul>"},{"location":"3_body/#virtualizacion-proxmox","title":"Virtualizaci\u00f3n - Proxmox","text":"<p>Proxmox Virtual Environment, com\u00fanmente conocido como Proxmox, es una plataforma de virtualizaci\u00f3n de c\u00f3digo abierto que permite la administraci\u00f3n y la implementaci\u00f3n de m\u00e1quinas virtuales (VM) y contenedores en un entorno integrado. Proxmox se utiliza para crear y gestionar entornos virtuales en servidores f\u00edsicos y es especialmente \u00fatil en entornos de centro de datos y en la administraci\u00f3n de servidores.</p> <p>Los siguientes son los conceptos clave y el principio de funcionamiento de Proxmox:</p> <ol> <li>Virtualizaci\u00f3n basada en KVM y contenedores: Proxmox utiliza dos tecnolog\u00edas de virtualizaci\u00f3n principales: KVM (Kernel-based Virtual Machine) para m\u00e1quinas virtuales y contenedores LXC (Linux Containers). Esto proporciona flexibilidad para ejecutar tanto VMs completas como contenedores ligeros en la misma plataforma.</li> <li>Interfaz web de gesti\u00f3n: Proxmox ofrece una interfaz web de administraci\u00f3n f\u00e1cil de usar llamada Proxmox Virtual Environment (PVE). A trav\u00e9s de esta interfaz, los administradores pueden gestionar recursos f\u00edsicos, crear VMs y contenedores, realizar copias de seguridad, monitorear el rendimiento y llevar a cabo muchas otras tareas relacionadas con la virtualizaci\u00f3n.</li> <li>Gesti\u00f3n centralizada de recursos: Proxmox permite aprovechar al m\u00e1ximo los recursos f\u00edsicos del servidor al proporcionar una gesti\u00f3n centralizada de CPU, memoria, almacenamiento y redes. Los recursos se pueden asignar de manera din\u00e1mica y compartida entre VMs y contenedores seg\u00fan sea necesario.</li> <li>Almacenamiento: Proxmox admite una variedad de opciones de almacenamiento, incluidos discos locales, almacenamiento compartido en red (NFS, Ceph, etc.) y almacenamiento en la nube. Esto permite a los administradores configurar soluciones de almacenamiento adecuadas para sus necesidades.</li> <li>Administraci\u00f3n de copias de seguridad: Proxmox incluye herramientas integradas para realizar copias de seguridad y restaurar m\u00e1quinas virtuales y contenedores. Los administradores pueden programar copias de seguridad autom\u00e1ticas y almacenar copias de seguridad en ubicaciones seguras.</li> <li>Administraci\u00f3n de cl\u00fasteres: Proxmox permite la creaci\u00f3n de cl\u00fasteres de servidores para mejorar la alta disponibilidad y la redundancia. Esto significa que las VMs y los contenedores pueden migrar de un nodo a otro en caso de fallos, lo que garantiza la continuidad del servicio.</li> <li>Seguridad y aislamiento: Proxmox se esfuerza por garantizar el aislamiento y la seguridad entre VMs y contenedores. Las tecnolog\u00edas de virtualizaci\u00f3n y contenedores se utilizan para asegurarse de que los sistemas en ejecuci\u00f3n no interfieran entre s\u00ed.</li> <li>Escalabilidad: Los administradores pueden agregar servidores adicionales al cl\u00faster Proxmox seg\u00fan sea necesario para aumentar la capacidad de procesamiento y almacenamiento de la plataforma.</li> </ol>"},{"location":"3_body/#aprovisionamiento","title":"Aprovisionamiento","text":"<p>En el contexto de sistemas de software y tecnolog\u00eda de la informaci\u00f3n, el t\u00e9rmino \"aprovisionamiento\" se refiere al proceso de configurar y suministrar recursos inform\u00e1ticos, como servidores, redes, almacenamiento y otros componentes de infraestructura, para satisfacer las necesidades de una aplicaci\u00f3n o servicio espec\u00edfico. El aprovisionamiento implica la asignaci\u00f3n de recursos de manera eficiente y escalable, de modo que los sistemas puedan funcionar de manera \u00f3ptima y satisfacer la demanda de los usuarios.</p> <p>El aprovisionamiento puede ser un proceso manual o automatizado, dependiendo de la complejidad de la infraestructura y de las herramientas disponibles. En entornos de nube, como Amazon Web Services (AWS), Microsoft Azure o Google Cloud Platform, el aprovisionamiento se realiza frecuentemente mediante servicios de aprovisionamiento autom\u00e1tico que permiten escalar los recursos de manera din\u00e1mica seg\u00fan las necesidades de la aplicaci\u00f3n. Esto es especialmente \u00fatil para garantizar que los sistemas sean flexibles y capaces de manejar cargas de trabajo variables.</p>"},{"location":"3_body/#de-infraestructura-terraform","title":"De Infraestructura - Terraform","text":"<p>Terraform es una herramienta de c\u00f3digo abierto desarrollada por HashiCorp que se utiliza para el aprovisionamiento y la gesti\u00f3n de infraestructura como c\u00f3digo (IaC, por sus siglas en ingl\u00e9s). IaC es una pr\u00e1ctica en la que la infraestructura se define y administra mediante c\u00f3digo, lo que permite automatizar y estandarizar la creaci\u00f3n y configuraci\u00f3n de recursos de infraestructura, como servidores, redes, bases de datos y otros componentes, de manera consistente y repetible.</p> <p>El principio de funcionamiento de Terraform se basa en los siguientes conceptos clave:</p> <ol> <li>Declaraci\u00f3n de infraestructura como c\u00f3digo (IaC): En Terraform, los usuarios definen la infraestructura deseada en archivos de configuraci\u00f3n escritos en un lenguaje espec\u00edfico llamado HashiCorp Configuration Language (HCL) o en formato JSON. En estos archivos, se describen los recursos necesarios, sus propiedades y sus dependencias.</li> <li>Configuraci\u00f3n declarativa: Terraform adopta un enfoque declarativo, lo que significa que los usuarios especifican lo que quieren lograr, pero no necesariamente c\u00f3mo hacerlo paso a paso. Terraform se encarga de determinar la secuencia de acciones necesarias para llevar la infraestructura actual al estado deseado.</li> <li>Planificaci\u00f3n y ejecuci\u00f3n: Despu\u00e9s de definir la configuraci\u00f3n, los usuarios ejecutan comandos de Terraform, como <code>terraform init</code>, <code>terraform plan</code> y <code>terraform apply</code>. El comando <code>plan</code> es especialmente \u00fatil, ya que muestra una vista previa de los cambios que Terraform realizar\u00e1 en la infraestructura actual para llevarla al estado deseado, sin aplicarlos de inmediato.</li> <li>Grafo de recursos: Terraform crea un grafo de recursos que representa las dependencias entre los recursos definidos en la configuraci\u00f3n. Esto permite que Terraform determine el orden en el que se deben crear o modificar los recursos para garantizar la coherencia de la infraestructura.</li> <li>Estado de infraestructura: Terraform mantiene un archivo de estado que registra el estado actual de la infraestructura gestionada. Este archivo se utiliza para realizar un seguimiento de los recursos creados y para determinar los cambios necesarios durante las ejecuciones posteriores de Terraform.</li> <li>Aplicaci\u00f3n y gesti\u00f3n de cambios: Una vez que los usuarios est\u00e1n satisfechos con la vista previa del plan de ejecuci\u00f3n, pueden aplicar los cambios utilizando el comando <code>apply</code>. Terraform se encarga de llevar la infraestructura al estado deseado, creando, actualizando o eliminando recursos seg\u00fan sea necesario.</li> </ol> <p>Terraform es altamente extensible y es compatible con una amplia variedad de proveedores de infraestructura, incluidos proveedores de nube como AWS, Azure, Google Cloud, as\u00ed como recursos locales, como servidores f\u00edsicos y m\u00e1quinas virtuales en centros de datos locales. Esto lo convierte en una herramienta poderosa para la gesti\u00f3n de infraestructura en entornos de nube, entornos locales o h\u00edbridos.</p>"},{"location":"3_body/#de-software-ansible","title":"De Software - Ansible","text":"<p>Ansible es una herramienta de automatizaci\u00f3n y gesti\u00f3n de configuraci\u00f3n de c\u00f3digo abierto que se utiliza para aprovisionar y administrar software en sistemas y servidores. A diferencia de Terraform, que se enfoca en la infraestructura subyacente, Ansible se centra en la configuraci\u00f3n y el mantenimiento de aplicaciones y servicios en sistemas ya aprovisionados. Ansible permite automatizar tareas como la instalaci\u00f3n de software, la configuraci\u00f3n de servidores, la actualizaci\u00f3n de aplicaciones y la gesti\u00f3n de configuraciones de manera eficiente y consistente.</p> <p>El principio de funcionamiento de Ansible se basa en los siguientes conceptos clave:</p> <ol> <li>Agentless: Ansible es una herramienta \"sin agente\", lo que significa que no es necesario instalar software adicional en los sistemas de destino para que Ansible funcione. En lugar de utilizar agentes permanentes, Ansible se comunica con los sistemas de destino a trav\u00e9s de SSH (para sistemas Unix/Linux) o WinRM (para sistemas Windows). Esto facilita la implementaci\u00f3n y la gesti\u00f3n de Ansible en una amplia variedad de entornos.</li> <li>Playbooks y Roles: Ansible utiliza Playbooks, que son archivos de texto YAML que describen las tareas que se deben realizar en los sistemas de destino. Los Playbooks son altamente legibles y permiten a los usuarios definir tareas espec\u00edficas, como instalar software, configurar archivos de configuraci\u00f3n y realizar otras acciones. Para promover la reutilizaci\u00f3n y la organizaci\u00f3n, las tareas se pueden agrupar en Roles, que son conjuntos de tareas relacionadas que se pueden aplicar a diferentes hosts.</li> <li>Declarativo: Ansible sigue un enfoque declarativo, lo que significa que los Playbooks describen el estado deseado del sistema en lugar de los pasos espec\u00edficos para llegar a ese estado. Ansible se encarga de determinar c\u00f3mo llevar los sistemas al estado deseado.</li> <li>M\u00f3dulos: Ansible incluye una amplia colecci\u00f3n de m\u00f3dulos que permiten realizar una variedad de tareas en los sistemas de destino. Estos m\u00f3dulos son responsables de ejecutar acciones espec\u00edficas, como instalar paquetes, copiar archivos, reiniciar servicios y m\u00e1s. Los usuarios pueden utilizar estos m\u00f3dulos en sus Playbooks para lograr sus objetivos de configuraci\u00f3n.</li> <li>Inventario: Ansible utiliza un archivo de inventario para definir los hosts (sistemas de destino) en los que se ejecutar\u00e1n las tareas. Los inventarios pueden ser est\u00e1ticos o din\u00e1micos, lo que permite gestionar grupos de hosts de manera flexible y escalable.</li> <li>Ejecuci\u00f3n: Para ejecutar un Playbook o una tarea de Ansible, los usuarios pueden utilizar el comando <code>ansible-playbook</code> o comandos similares. Ansible se encarga de conectarse a los sistemas de destino, aplicar las tareas definidas en el Playbook y garantizar que el sistema est\u00e9 en el estado deseado.</li> </ol>"},{"location":"3_body/#uso-en-conjunto-terraform-ansible","title":"Uso en conjunto - Terraform + Ansible","text":"<p>Terraform y Ansible son dos herramientas complementarias que se utilizan com\u00fanmente juntas para gestionar de manera integral tanto la infraestructura como la configuraci\u00f3n de software en un entorno de TI. La combinaci\u00f3n de ambas herramientas permite automatizar todo el ciclo de vida de un sistema, desde la creaci\u00f3n y aprovisionamiento de la infraestructura hasta la configuraci\u00f3n y administraci\u00f3n de aplicaciones y servicios en dicha infraestructura. Aqu\u00ed hay algunas formas t\u00edpicas en las que se utilizan Terraform y Ansible juntos:</p> <ol> <li>Provisionamiento de infraestructura con Terraform: Terraform se utiliza para crear y aprovisionar la infraestructura subyacente, como servidores, redes, bases de datos y otros recursos en la nube o en centros de datos locales. Terraform puede configurar la topolog\u00eda de la infraestructura y asegurarse de que los recursos est\u00e9n disponibles y funcionando seg\u00fan lo previsto.</li> <li>Configuraci\u00f3n de servidores y aplicaciones con Ansible: Una vez que Terraform ha creado la infraestructura, Ansible se encarga de configurar y gestionar los servidores y aplicaciones en esa infraestructura. Ansible automatiza tareas como la instalaci\u00f3n de software, la configuraci\u00f3n de archivos de configuraci\u00f3n, la aplicaci\u00f3n de parches y la gesti\u00f3n de servicios en los servidores.</li> <li>Orquestaci\u00f3n completa de aplicaciones: Terraform y Ansible se pueden utilizar en conjunto para orquestar la implementaci\u00f3n de aplicaciones completas. Terraform puede aprovisionar servidores y recursos, y luego Ansible puede configurar y administrar la aplicaci\u00f3n en esos servidores, garantizando que todo el entorno est\u00e9 funcionando correctamente.</li> <li>Actualizaciones y cambios en la infraestructura: Cuando es necesario realizar cambios en la infraestructura, como agregar o quitar servidores, Terraform se encarga de modificar la infraestructura de manera controlada y segura. Luego, Ansible puede aplicar las configuraciones necesarias en los nuevos recursos o realizar ajustes en los recursos existentes para acomodar los cambios.</li> <li>Automatizaci\u00f3n de despliegues y escalabilidad: Terraform y Ansible permiten escalar la infraestructura y las aplicaciones de manera automatizada en funci\u00f3n de la demanda. Por ejemplo, cuando se necesita escalar una aplicaci\u00f3n web, Terraform puede agregar nuevos servidores, y Ansible puede configurarlos autom\u00e1ticamente para unirse al cl\u00faster de aplicaciones existente.</li> <li>Gesti\u00f3n de la configuraci\u00f3n continua: Ansible se utiliza para garantizar que la configuraci\u00f3n de software se mantenga coherente a lo largo del tiempo. Puede aplicar pol\u00edticas de configuraci\u00f3n y asegurarse de que los servidores y las aplicaciones cumplan con los est\u00e1ndares de seguridad y rendimiento.</li> </ol>"},{"location":"3_body/#como-se-comunican-terraform-y-ansible-para-poder-realizar-las-configuraciones","title":"\u00bfC\u00f3mo se comunican Terraform y Ansible para poder realizar las configuraciones?","text":"<p>Tanto Terraform como Ansible son herramientas de automatizaci\u00f3n que pueden comunicarse con otros sistemas y recursos utilizando diferentes protocolos y mecanismos de autenticaci\u00f3n. A continuaci\u00f3n, se describen los protocolos y m\u00e9todos de autenticaci\u00f3n comunes utilizados por cada una de estas herramientas:</p> <p>Terraform:</p> <ol> <li> <p>APIs de proveedores de nube: Terraform se comunica con los proveedores de nube (como AWS, Azure, Google Cloud, etc.) a trav\u00e9s de sus respectivas APIs. Estas APIs suelen utilizar protocolos basados en HTTP/HTTPS, como REST o gRPC, para la comunicaci\u00f3n. Terraform utiliza las credenciales del proveedor de nube (por ejemplo, las claves de acceso de AWS) para autenticarse y realizar operaciones en la nube.</p> </li> <li> <p>SSH: En ocasiones, Terraform puede utilizar SSH para comunicarse con m\u00e1quinas virtuales o servidores provisionados a trav\u00e9s de SSH. En este caso, se utilizan pares de claves SSH (p\u00fablica y privada) para la autenticaci\u00f3n. La clave p\u00fablica se coloca en el servidor de destino, y la clave privada se utiliza para autenticar la conexi\u00f3n desde Terraform.</p> </li> </ol> <p>Ansible:</p> <ol> <li> <p>SSH: Ansible utiliza SSH como el protocolo predeterminado para conectarse a los servidores de destino y ejecutar comandos o tareas en ellos. Al igual que con Terraform, Ansible utiliza pares de claves SSH (p\u00fablica y privada) para la autenticaci\u00f3n. La clave p\u00fablica se debe agregar a los servidores de destino y la clave privada se utiliza para autenticar las conexiones desde la m\u00e1quina que ejecuta Ansible.</p> </li> <li> <p>WinRM: Para sistemas Windows, Ansible puede utilizar WinRM (Windows Remote Management) como protocolo de comunicaci\u00f3n en lugar de SSH. WinRM permite la administraci\u00f3n remota de servidores Windows y utiliza autenticaci\u00f3n basada en credenciales (nombre de usuario y contrase\u00f1a) o certificados.</p> </li> </ol> <p>En cuanto a la comunicaci\u00f3n con Proxmox, tanto Terraform como Ansible pueden utilizar SSH para conectarse a los nodos de Proxmox y administrarlos, ya que Proxmox es compatible con SSH para la administraci\u00f3n remota. En este caso, se requerir\u00e1n las claves SSH adecuadas y las configuraciones de acceso para autenticarse en los nodos de Proxmox.</p>"},{"location":"3_body/#orquestacion-de-contenedores-kubernetes","title":"Orquestaci\u00f3n de contenedores - Kubernetes","text":"<p>Kubernetes, a menudo abreviado como K8s, es una plataforma de c\u00f3digo abierto dise\u00f1ada para la automatizaci\u00f3n, la implementaci\u00f3n, la escalabilidad y la administraci\u00f3n de aplicaciones en contenedores. Kubernetes es ampliamente utilizado en el despliegue y la gesti\u00f3n de aplicaciones en entornos de nube y en centros de datos locales. Su principal objetivo es facilitar la orquestaci\u00f3n de contenedores, lo que permite administrar aplicaciones de manera eficiente y escalable.</p> <p>Los siguientes son los conceptos clave y el principio de funcionamiento de Kubernetes:</p> <ol> <li>Contenedores: Kubernetes se basa en la tecnolog\u00eda de contenedores, que permite empaquetar aplicaciones y sus dependencias en entornos aislados y port\u00e1tiles. Los contenedores son ligeros, r\u00e1pidos de implementar y pueden ejecutarse de manera consistente en cualquier entorno compatible con contenedores.</li> <li>Orquestaci\u00f3n: Kubernetes se utiliza para orquestar la implementaci\u00f3n y la administraci\u00f3n de contenedores en cl\u00fasteres de servidores. Esto significa que Kubernetes automatiza tareas como la distribuci\u00f3n de contenedores en servidores, la recuperaci\u00f3n ante fallos, la escalabilidad de aplicaciones y la gesti\u00f3n de recursos.</li> <li>Cl\u00faster de Kubernetes: Un cl\u00faster de Kubernetes es un conjunto de nodos (servidores) que ejecutan el software de Kubernetes y que trabajan juntos para administrar aplicaciones en contenedores. Los cl\u00fasteres pueden incluir nodos maestros (control plane) y nodos de trabajo (worker nodes).</li> <li>Nodos maestros: Los nodos maestros son responsables de la gesti\u00f3n y el control del cl\u00faster de Kubernetes. Se encargan de tomar decisiones sobre la programaci\u00f3n de contenedores, la asignaci\u00f3n de recursos, la gesti\u00f3n de la alta disponibilidad y la administraci\u00f3n de la configuraci\u00f3n del cl\u00faster.</li> <li>Nodos de trabajo: Los nodos de trabajo son los servidores donde se ejecutan los contenedores. Cada nodo de trabajo tiene un agente de Kubernetes llamado kubelet que se comunica con el nodo maestro y administra la ejecuci\u00f3n de los contenedores en el nodo.</li> <li>Despliegues y Pods: Kubernetes utiliza abstracciones como Despliegues (Deployments) y Pods para definir y gestionar aplicaciones. Los Despliegues especifican c\u00f3mo se deben ejecutar los Pods (que son la unidad m\u00e1s peque\u00f1a en Kubernetes) y controlan la escalabilidad, la actualizaci\u00f3n y el equilibrio de carga de las aplicaciones.</li> <li>Servicios: Kubernetes ofrece Servicios para exponer aplicaciones y servicios a trav\u00e9s de una red. Los Servicios permiten la comunicaci\u00f3n entre Pods y garantizan la disponibilidad incluso cuando los Pods se escalan o cambian de ubicaci\u00f3n.</li> <li>Escalabilidad y autorrecuperaci\u00f3n: Kubernetes permite escalar aplicaciones autom\u00e1ticamente seg\u00fan la demanda y recuperarse de fallos de forma autom\u00e1tica. Esto garantiza que las aplicaciones sean altamente disponibles y capaces de manejar cargas de trabajo variables.</li> <li>Configuraci\u00f3n declarativa: En Kubernetes, los usuarios definen el estado deseado de las aplicaciones y de la infraestructura a trav\u00e9s de archivos de configuraci\u00f3n YAML o JSON. Kubernetes se encarga de llevar el sistema al estado deseado y mantenerlo de esa manera.</li> </ol>"},{"location":"3_body/#despliegue-administracion-y-la-orquestacion-de-flujos-de-trabajo-de-ml-kubeflow","title":"Despliegue, administraci\u00f3n y la orquestaci\u00f3n de flujos de trabajo de ML - Kubeflow","text":"<p>Kubeflow es una plataforma de c\u00f3digo abierto dise\u00f1ada espec\u00edficamente para el despliegue, la administraci\u00f3n y la orquestaci\u00f3n de flujos de trabajo de aprendizaje autom\u00e1tico (machine learning, ML) en Kubernetes. Esta plataforma se cre\u00f3 para facilitar el desarrollo, la capacitaci\u00f3n y la implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico de manera eficiente y escalable en entornos basados en contenedores, aprovechando las ventajas de Kubernetes para la orquestaci\u00f3n y la gesti\u00f3n de recursos.</p> <p>Los siguientes son los conceptos clave y el principio de funcionamiento de Kubeflow:</p> <ol> <li> <p>Gesti\u00f3n de flujos de trabajo de ML: Kubeflow se centra en la gesti\u00f3n y la automatizaci\u00f3n de flujos de trabajo de ML, que involucran m\u00faltiples etapas, desde la recopilaci\u00f3n de datos y la preparaci\u00f3n de datos hasta el entrenamiento de modelos y la implementaci\u00f3n de modelos en producci\u00f3n.</p> </li> <li> <p>Soporte para contenedores: Kubeflow se integra perfectamente con la naturaleza basada en contenedores de Kubernetes. Los flujos de trabajo de ML se empaquetan en contenedores, lo que facilita la portabilidad y la implementaci\u00f3n en cualquier cl\u00faster de Kubernetes.</p> </li> <li> <p>Componentes modulares: Kubeflow se compone de varios componentes y herramientas que se pueden utilizar de manera modular seg\u00fan las necesidades espec\u00edficas de un proyecto de aprendizaje autom\u00e1tico. Algunos de los componentes clave incluyen Katib (para la optimizaci\u00f3n de hiperpar\u00e1metros), KFServing (para la implementaci\u00f3n de modelos), Pipelines (para la creaci\u00f3n y la gesti\u00f3n de flujos de trabajo) y m\u00e1s.</p> </li> <li> <p>Orquestaci\u00f3n de flujos de trabajo: Kubeflow Pipelines es una de las caracter\u00edsticas principales que permite a los usuarios definir y ejecutar flujos de trabajo de ML como secuencias de pasos interconectados. Cada paso puede ser una tarea de preparaci\u00f3n de datos, entrenamiento de modelos, evaluaci\u00f3n de modelos o implementaci\u00f3n en producci\u00f3n. Los flujos de trabajo son configurables y reproducibles.</p> </li> <li> <p>Automatizaci\u00f3n y escalabilidad: Kubeflow simplifica la automatizaci\u00f3n de flujos de trabajo, lo que significa que los flujos de trabajo se pueden ejecutar de manera program\u00e1tica en respuesta a eventos o programarse para ejecutarse de manera regular. Adem\u00e1s, Kubernetes facilita la escalabilidad de recursos para manejar grandes vol\u00famenes de datos y entrenamientos de modelos intensivos en recursos.</p> </li> <li> <p>Gesti\u00f3n de modelos: Kubeflow ofrece herramientas para el registro, la versi\u00f3n y la administraci\u00f3n de modelos de aprendizaje autom\u00e1tico. Esto facilita la colaboraci\u00f3n y la gesti\u00f3n de modelos a lo largo de su ciclo de vida.</p> </li> <li> <p>Monitorizaci\u00f3n y seguimiento: Kubeflow incluye capacidades de monitoreo y seguimiento que permiten a los usuarios evaluar el rendimiento de los modelos implementados en producci\u00f3n y realizar ajustes seg\u00fan sea necesario.</p> </li> </ol> <p>En resumen, Kubeflow es una plataforma de c\u00f3digo abierto que simplifica y optimiza la gesti\u00f3n de flujos de trabajo de aprendizaje autom\u00e1tico en entornos de Kubernetes. Facilita la implementaci\u00f3n, la escalabilidad, la automatizaci\u00f3n y la gesti\u00f3n de modelos de ML en entornos basados en contenedores, lo que mejora la eficiencia y la reproducibilidad en proyectos de aprendizaje autom\u00e1tico.</p>"},{"location":"4_conclusion/","title":"Conclusi\u00f3n","text":"<p>Vivamus a magna et tellus egestas accumsan vel nec nisl. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum sit amet magna nisi. Proin pellentesque eros vel est vestibulum, sit amet venenatis lectus venenatis. Curabitur magna est, efficitur congue pulvinar sit amet, accumsan sit amet arcu. Phasellus vitae viverra turpis. Aliquam nec convallis lorem. Vivamus vel fringilla leo. Nulla sit amet neque ornare, bibendum ex quis, fermentum massa. Aliquam ac sem diam. Curabitur ullamcorper porta tortor. Nunc justo turpis, imperdiet id luctus consectetur, vulputate cursus eros. Integer aliquet dolor velit, eu pulvinar odio fringilla nec. Maecenas vitae massa auctor, commodo risus eu, venenatis nibh. Cras vulputate molestie posuere.</p>"},{"location":"5_bibliography/","title":"Bibliograf\u00eda","text":"<ul> <li> <p>How to Manage Releases with Semantic Versioning and Git Tags</p> </li> <li> <p>4 branching workflows for Git</p> </li> <li> <p>GitHub flow - GitHub Docs</p> </li> <li> <p>Vagrant ssh key pair - DevopsRoles.com top 1.</p> </li> <li> <p>Install and Specify a Box | Vagrant | HashiCorp Developer.</p> </li> <li> <p>Introduction to DVC and MLflow for Experiment tracking. Valohai.</p> </li> <li> <p>A Comprehensive Comparison Between Kubeflow and Airflow. Valohai.</p> </li> <li> <p>A Comprehensive Comparison Between Kubeflow and MLflow. Valohai.</p> </li> <li> <p>Airflow, MLflow or Kubeflow for MLOps?. AICurious.</p> </li> <li> <p>Kubernetes Setup Using Ansible and Vagrant</p> </li> <li> <p>GitHub - AudelDiaz - kubeadm-cluster</p> </li> <li> <p>Ansible Best Practicas - Directory Layout</p> </li> </ul>"},{"location":"6_attachments/","title":"Anexo","text":""},{"location":"6_attachments/#semantic-releases","title":"Semantic Releases","text":""},{"location":"6_attachments/#que-es-una-release","title":"\u00bfQu\u00e9 es una \u2018release\u2019?","text":"<p>Una release es empaquetar cualquier cambio en el c\u00f3digo y enviarlo a producci\u00f3n. Por ejemplo, un cambio de nuestra p\u00e1gina web que vaya al p\u00fablico y no a nuestra etapa de desarrollo.</p> <p>El manejo de estas releases puede ser un poco complicado, especialmente si no seguimos un cierto standard. Por eso es que usamos \u2018semantic versioning\u2019 con git tags para manejar de manera f\u00e1cil nuestras releases.</p>"},{"location":"6_attachments/#que-es-el-semantic-versioning","title":"\u00bfQu\u00e9 es el \u2018semantic versioning\u2019?","text":"<p>El semantic versioning es s\u00f3lo un esqueman num\u00e9rico, es una pr\u00e1ctica est\u00e1ndar de la industria del software que sirve para indicar el \u201cgrado de cambios\u201d que se han hecho desde la release de producci\u00f3n anterior. Todos usan semantic versioning, desde Git, hasta Firefox y los SO como iOS.</p>"},{"location":"6_attachments/#que-estructura-tiene-la-semantic-versioning","title":"\u00bfQu\u00e9 estructura tiene la semantic versioning?","text":"<p>Tiene 3 partes:</p> <pre><code>MAJOR.MINOR.PATCH\n</code></pre> <p>Cada una de las partes indica algo diferente cuando incrementa:</p> <ul> <li> <p>Major: Cuando vamos de 1.0.0 a 2.0.0 indicamos que cambiamos de manera disruptiva, incluimos cambios que no sean compatibles hacia atr\u00e1s, etc. Por ejemplo, remover c\u00f3digo que ya no sirve para incluir una reestructuraci\u00f3n completa de la arquitectura de nuestra aplicaci\u00f3n.</p> </li> <li> <p>Minor: Cuando vamos de 1.0.1 a 1.1.0 indicamos que cambiamos funcionalidades, pero que estos cambios son compatibles hacia atr\u00e1s. Puede ser el cambio de una funcionalidad, la actualizaci\u00f3n de una, el agregado de otra.</p> </li> <li> <p>Patch:  Cuando vamos de 1.0.1 a 1.0.2 indicamos arreglos de bugs y actualizaciones triviales.</p> </li> </ul>"},{"location":"6_attachments/#premisas-del-semantic-versioning","title":"Premisas del semantic versioning","text":"<ul> <li>Una vez hecha una release, la versi\u00f3n no puede ser cambiada</li> <li>Si nos olvidamos algo no podemos \u201cretaggear\u201d una versi\u00f3n, estos deber\u00edan entrar en una nueva release.</li> <li>Somos responsables de checkear reiteradamente la versi\u00f3n actual antes de hacer un release.</li> </ul>"},{"location":"6_attachments/#git-tagging-que-es-un-tag","title":"Git Tagging \u00bfQu\u00e9 es un Tag?","text":"<p>Es una manera de agregar un marcador o marker a un commit para indicar que es importante de alguna manera en nuestro repositorio. Hay dos diferentes tipos de GitTags:</p> <ul> <li> <p>Lightweigh tags: Un puntero con nombre b\u00e1sico para un commit.</p> <pre><code>&gt; git tag &lt;tag-name&gt; [commit]\n</code></pre> </li> <li> <p>Annotated tags: Un objeto completo en la database de git verificado, contiene informaci\u00f3n de el tag, tiene un mensaje de taggeo (tagging message) y puede ser firmada y verificada con GNU Privacy Guard (GPG). Esta \u00faltima es la que se recomienda usar.</p> <pre><code>&gt; git tag -a &lt;tag-name&gt; -m\"&lt;annotation&gt;\" [commit]\n</code></pre> </li> </ul> <p>Tanto el semantic versioning como el GitTagging van de la mano, podemos agregar un commit taggeando la versi\u00f3n correspondiente.</p>"},{"location":"6_attachments/#semantic-versioning-annotated-tags-semantic-releases","title":"Semantic versioning + Annotated Tags = Semantic Releases","text":"<p>Nos permite tener commits marcados en nuestro repositorio de git con una versi\u00f3n espec\u00edfica. Los beneficios de esto en un repositorio de git son:</p> <ul> <li>Le da significado a los cambios importantes en nuestro repositorio.</li> <li>Comunica el \u201cgrado de cambio\u201d entre los diferentes tags.</li> <li>Vemos de manera directa el historial de tracking de los cambios realizados.</li> </ul>"},{"location":"6_attachments/#por-que-plataformas-o-herramientas-esta-soportado-esto","title":"\u00bfPor qu\u00e9 plataformas o herramientas est\u00e1 soportado esto?","text":"<ul> <li>Diferentes interfaces de Git, como Git Kraken o GitHub Desktop.</li> <li>Diferentes herramientas de automatizaci\u00f3n como Circle CI, Bitbucket, Travis, etc.</li> </ul>"},{"location":"6_attachments/#como-creo-las-semantic-git-releases","title":"\u00bfC\u00f3mo creo las 'Semantic Git Releases'?","text":"<p>Es un proceso que consiste en 3 pasos:</p> <ol> <li>Crear un annotated tag<ol> <li>Usar semantic versioning para el nombre del tag</li> <li>Brindar una annotation</li> </ol> </li> <li>Pushear el tag al repositorio remoto</li> <li>Insertar los pasos de deployment ac\u00e1</li> </ol> <p>Crear una un semantic release tag usando la consola:</p> <pre><code>&gt; git tag -a v1.0.0 -m \"release 1.0.0\"\n&gt; git push &lt;remote&gt; v1.0.0\n</code></pre>"},{"location":"6_attachments/#release-notes","title":"Release Notes","text":"<p>Tenemos que evitar las anotaciones m\u00ednimas. Se recomienda, para cada tipo de release:</p> <ul> <li>Patch: Lista de los bug fixes</li> <li>Minor: Lista de cambios, detalles de uso.</li> <li>Major: Lista de elementos removidos, lista de cosas agregadas, proceso de actualizaci\u00f3n.</li> </ul> <p>Tomar una lista de los mensajes de los commits entre releases:</p> <pre><code>git log --pretty=format:%s &lt;last release&gt;... HEAD --no-merges\n\ngit tag -a &lt;tag-name&gt; -m\"$(git log --pretty=format:%s &lt;last release&gt;... HEAD --no-merges)\"\n</code></pre>"},{"location":"6_attachments/#como-automatizo-la-generacion-de-los-tags","title":"\u00bfC\u00f3mo automatizo la generaci\u00f3n de los tags?","text":"<ul> <li>Puedo buscar en el mercado por alguna herramienta de automatizaci\u00f3n.</li> <li>Crear un script de bash por nosotros mismos para ayudarnos a automatizar los pasos repetitivos.</li> </ul>"},{"location":"6_attachments/#git-workflow","title":"Git Workflow","text":"<p>Los Git Workflows son metodolog\u00edas de trabajo para los usuarios de de Git.  </p>"},{"location":"6_attachments/#git-flow","title":"Git Flow","text":"<p>Es el workflow m\u00e1s conocido, basado en dos branches principales que son perpetuas, con vida infinita. Estas son:</p> <ul> <li>master: Tiene el c\u00f3digo de producci\u00f3n. Todo el c\u00f3digo de desarrollo es \u2018mergeado\u2019 dentro de la branch master en alg\u00fan momento.</li> <li>develop:  Contiene el c\u00f3digo de pre-producci\u00f3n. Cuando las modificacioens o nuevas caracter\u00edsticas est\u00e9n finalizadas, se \u2018mergean\u2019 en la branch develop.</li> </ul> <p>Durante el ciclo de desarrollo, una variedad de ramas de soporte son utilizadas:</p> <ul> <li>feature-*: Usada para desarrollar nuevas caracter\u00edsitcas que vendr\u00e1n en las futuras releases. Deber\u00eda desprenderse de la rama develop y mergearse en la rama develop.</li> <li>hotfix-: Son necesarias para actuar inmediatamente ante un estado indeseado de la branch master. Deber\u00eda desprenderse de la branch master y mergearse tanto en m\u00e1ster como en develop.</li> <li>release-*: Son la preparaci\u00f3n de una nueva release de producci\u00f3n. Permiten que haya menos bugs que arreglar y la preparaci\u00f3n de la metadata para la release. Debe desprenderse de la rama develop y debe ser mergeada en la rama master y develop.</li> </ul>"},{"location":"6_attachments/#github-flow","title":"GitHub Flow","text":"<p>Es un workflow liviano creado por GitHub y se basa en 6 principios:</p> <ol> <li>Todo en la rama master es deployable.</li> <li>Para trabajar en algo nuevo, creamos una rama desde la master con un nombre descriptivo.</li> <li>Hacemos commit a esa rama localmente y regularmente hacemos push del trabajo a la misma rama en remoto.</li> <li>Cuando necesitamos feedback o creemos que es necesario mergear, abrimos un Pull Request (PR).</li> <li>Despues de que alguien haya revisado y firmado la nueva caracter\u00edstica, se puede hacer merge en la master.</li> <li>Una vez hecho el merge y pusheado a la rama master, debemos deployar inmediatamente.</li> </ol>"},{"location":"6_attachments/#gitlab-flow","title":"GitLab Flow","text":"<p>Es un workflow creado por GitLab. Combina un desarrollo dirigido por las funcionalidades (caracteristicas) y con ramas de funcionalidades con un trackeo de problemas.</p> <p>La mayor diferencia con GitHub Flow es el ambiente de las ramas que tenemos en GitLab Flow (staging y production) porque ser\u00e1 un proyecto que no puede deployarse en producci\u00f3n cada vez que hacemos un merge de una nueva feature branch. Se basa en 11 principios:</p> <ol> <li>Usa feature branches, no commits directos a master.</li> <li>Prueba todos los commits, no solo los de la master.</li> <li>Corre todos los test en todos los commits.</li> <li>Hacer revisi\u00f3n de codigo antes de hacer el merge en master.</li> <li>Los deployments son autom\u00e1ticos, basados en las branches o tags.</li> <li>Los tags son configurados por el usuario, no por el CI.</li> <li>Las releases son basadas en tags.</li> <li>Los commits ya pusheados nunca son rebasados.</li> <li>Todos comienzan por master y apuntan a master.</li> <li>Corregir bugs en master primero, release branches segundo.</li> <li>Los commits reflejan la intenci\u00f3n.</li> </ol>"},{"location":"6_attachments/#cual-elegimos","title":"\u00bfCual elegimos?","text":"<p>Por simplicidad y por la plataforma en la que estamos trabajando el workflow m\u00e1s conveniente ser\u00e1 GitHub Workflow.</p>"},{"location":"6_attachments/#metodologias-agiles","title":"Metodolog\u00edas \u00e1giles","text":""},{"location":"6_attachments/#manifiesto-agil","title":"Manifiesto \u00e1gil","text":"<p>El Manifiesto \u00c1gil es un documento que establece los valores y principios fundamentales para el desarrollo \u00e1gil de software. Fue creado en 2001 por un grupo de expertos en desarrollo de software que buscaban alternativas m\u00e1s flexibles y eficientes a los enfoques tradicionales de gesti\u00f3n de proyectos.</p>"},{"location":"6_attachments/#valores-del-manifiesto-agil","title":"Valores del Manifiesto \u00c1gil","text":"<ol> <li>Individuos e interacciones sobre procesos y herramientas: Se enfoca en la importancia de las personas y la comunicaci\u00f3n efectiva en el desarrollo de software.</li> <li>Software funcionando sobre documentaci\u00f3n extensiva: Prioriza la entrega de software funcional y utilizable por encima de una documentaci\u00f3n exhaustiva.</li> <li>Colaboraci\u00f3n con el cliente sobre negociaci\u00f3n contractual: Destaca la importancia de la colaboraci\u00f3n continua con el cliente para adaptarse a los cambios y requisitos emergentes.</li> <li>Responder a cambios sobre seguir un plan: Aboga por la flexibilidad y la capacidad de adaptarse a cambios en los requisitos, incluso en etapas avanzadas del desarrollo.</li> </ol>"},{"location":"6_attachments/#principios-del-manifiesto-agil","title":"Principios del Manifiesto \u00c1gil","text":"<ol> <li>La m\u00e1s alta prioridad es satisfacer al cliente mediante la entrega temprana y continua de software valioso.</li> <li>Aceptar cambios en los requisitos, incluso en etapas tard\u00edas del desarrollo.</li> <li>Entregar software funcional con frecuencia, con preferencia a intervalos cortos.</li> <li>Colaborar con clientes y usuarios a lo largo del proyecto.</li> <li>Construir proyectos alrededor de individuos motivados, d\u00e1ndoles el entorno y el apoyo que necesitan y confiando en ellos para que hagan el trabajo.</li> <li>El m\u00e9todo m\u00e1s eficiente y efectivo de comunicaci\u00f3n es la conversaci\u00f3n cara a cara.</li> <li>El software funcional es la principal medida de progreso.**</li> <li>Mantenerte enfocado en la simplicidad, maximizando la cantidad de trabajo no realizado.</li> <li>Las mejores arquitecturas, requisitos y dise\u00f1os surgen de equipos autoorganizados.</li> <li>A intervalos regulares, el equipo reflexiona sobre c\u00f3mo ser m\u00e1s efectivo y ajusta su comportamiento en consecuencia.</li> </ol> <p>Hoy en d\u00eda, el Manifiesto \u00c1gil sigue siendo un marco de referencia influyente para el desarrollo de software. Las metodolog\u00edas \u00e1giles como Scrum, Kanban y XP (eXtreme Programming) se basan en estos valores y principios. Las organizaciones adoptan enfoques \u00e1giles para mejorar la flexibilidad, la capacidad de respuesta a cambios y la entrega continua de software de alta calidad. Adem\u00e1s, la cultura \u00e1gil ha trascendido el \u00e1mbito del desarrollo de software y se ha extendido a otras \u00e1reas como la gesti\u00f3n de proyectos, el marketing y la gesti\u00f3n empresarial.</p>"},{"location":"6_attachments/#scrum-roles-y-responsabilidades","title":"Scrum: Roles y responsabilidades","text":"<p>Scrum es un marco de trabajo \u00e1gil que se utiliza com\u00fanmente en el desarrollo de software para gestionar proyectos de manera iterativa e incremental. Los roles en Scrum son esenciales para la colaboraci\u00f3n y la entrega efectiva de productos. Los roles principales en Scrum y sus responsabilidades son:</p> <ol> <li>Product Owner (Due\u00f1o del Producto):<ul> <li>Responsabilidades:<ul> <li>Define la visi\u00f3n del producto.</li> <li>Prioriza el backlog del producto.</li> <li>Asegura que el equipo est\u00e9 trabajando en las caracter\u00edsticas m\u00e1s valiosas y prioritarias.</li> <li>Toma decisiones sobre el alcance y las caracter\u00edsticas del producto.</li> </ul> </li> </ul> </li> <li>Scrum Master (Facilitador del Proceso):<ul> <li>Responsabilidades:<ul> <li>Garantiza que el equipo Scrum siga las pr\u00e1cticas y reglas de Scrum.</li> <li>Facilita las reuniones del equipo, como las reuniones de planificaci\u00f3n, revisi\u00f3n y retrospectiva.</li> <li>Elimina los obst\u00e1culos que impiden el progreso del equipo.</li> <li>Ayuda a mantener un entorno de trabajo colaborativo y centrado en la entrega de valor.</li> </ul> </li> </ul> </li> <li>Equipo de Desarrollo:<ul> <li>Responsabilidades:<ul> <li>Desarrolla el producto durante los sprints.</li> <li>Colabora en la planificaci\u00f3n del sprint y define las tareas necesarias.</li> <li>Se autoorganiza para lograr los objetivos del sprint.</li> <li>Participa en las ceremonias de Scrum, como las reuniones diarias de scrum, la revisi\u00f3n y la retrospectiva.</li> </ul> </li> </ul> </li> </ol> <p>Es importante destacar que en Scrum, se fomenta la colaboraci\u00f3n y la autogesti\u00f3n del equipo. El Product Owner y el Scrum Master sirven al equipo y trabajan en conjunto para asegurar que el producto se desarrolle de manera efectiva y que se cumplan los objetivos del negocio. Adem\u00e1s, Scrum promueve la transparencia, la inspecci\u00f3n y la adaptaci\u00f3n continua, lo que permite a los equipos responder r\u00e1pidamente a los cambios en los requisitos o en el entorno del proyecto.</p> <p>En Scrum, tanto el \"Backlog\" como el \"Sprint\" son conceptos fundamentales que contribuyen al enfoque iterativo e incremental del desarrollo de software.</p> <ol> <li> <p>Backlog:</p> <p>El \"Product Backlog\" (Backlog del Producto) es una lista din\u00e1mica y priorizada de todas las funcionalidades, mejoras y tareas que podr\u00edan ser realizadas para un producto. Es responsabilidad del Product Owner mantener y gestionar este backlog. Algunas caracter\u00edsticas del Product Backlog incluyen:</p> <ul> <li>Priorizaci\u00f3n: Las \u00edtems del backlog est\u00e1n ordenados por prioridad, con las caracter\u00edsticas m\u00e1s importantes o de mayor valor para el cliente en la parte superior.</li> <li>Flexibilidad: El backlog es flexible y puede cambiar con el tiempo para adaptarse a las necesidades cambiantes del negocio o del cliente.</li> <li>Detalles: Los elementos del backlog no necesitan estar detallados en exceso. Los detalles se refinan a medida que los elementos se acercan al tope del backlog y se preparan para ser incluidos en un sprint.</li> <li>Sprint:</li> </ul> <p>Un \"Sprint\" es una unidad de tiempo fija y corta durante la cual se realiza un trabajo espec\u00edfico y se produce una versi\u00f3n potencialmente entregable del producto. Los sprints en Scrum generalmente tienen una duraci\u00f3n de dos a cuatro semanas. Algunas caracter\u00edsticas clave del Sprint incluyen:</p> <ul> <li>Objetivo del Sprint: Antes de comenzar un Sprint, el equipo selecciona elementos del Product Backlog para incluir en el Sprint Backlog, que es la lista de elementos que se comprometen a completar durante el sprint.</li> <li>Iterativo e Incremental: El trabajo se realiza en ciclos iterativos, y al final de cada Sprint, se produce un incremento potencialmente entregable del producto.</li> <li>Reuniones: Durante el sprint, el equipo se re\u00fane diariamente en la \"Daily Scrum\" para revisar el progreso y planificar el trabajo del d\u00eda.</li> <li>Revisi\u00f3n y Retrospectiva: Al final del Sprint, se llevan a cabo la \"Sprint Review\" para presentar el trabajo completado y obtener retroalimentaci\u00f3n, y la \"Sprint Retrospective\" para analizar el proceso y mejorar en futuros sprints.</li> </ul> </li> </ol> <p>La combinaci\u00f3n del Backlog y los Sprints permite a los equipos Scrum mantener un enfoque \u00e1gil y responder r\u00e1pidamente a los cambios en los requisitos del cliente o del negocio, al tiempo que entrega de manera regular incrementos de valor al producto.</p> <p>Para nuestro caso, el tutor docente actuar\u00eda como Scrum Master, el tutor externo actuar\u00eda de Product Owner y el equipo de desarrollo estar\u00eda conformado por el alumno. Adem\u00e1s hemos definido los sprints de una semana, y tomaremos tareas del backlog que se ha conformado a partir del plan de trabajo de esta PPS.</p>"},{"location":"6_attachments/#cultura-devops","title":"Cultura DevOps","text":"<p>La cultura DevOps es una filosof\u00eda y pr\u00e1ctica que promueve la colaboraci\u00f3n estrecha y continua entre los equipos de desarrollo (Dev) y operaciones (Ops) en el ciclo de vida del desarrollo de software. El objetivo principal es superar las barreras tradicionales entre estos dos departamentos para lograr una entrega de software m\u00e1s r\u00e1pida, eficiente y confiable.</p>"},{"location":"6_attachments/#principios-y-valores-de-la-cultura-devops","title":"Principios y Valores de la Cultura DevOps","text":"<ol> <li>Colaboraci\u00f3n y Comunicaci\u00f3n:<ul> <li>Principio: Fomentar la colaboraci\u00f3n abierta y una comunicaci\u00f3n efectiva entre los equipos de desarrollo y operaciones.</li> </ul> </li> <li>Automatizaci\u00f3n:<ul> <li>Principio: Automatizar tanto como sea posible los procesos de desarrollo, prueba y despliegue para mejorar la eficiencia y reducir errores.</li> </ul> </li> <li>Entrega Continua:<ul> <li>Principio: Buscar la entrega continua de software, permitiendo versiones peque\u00f1as y frecuentes en lugar de despliegues masivos y menos frecuentes.</li> </ul> </li> <li>Monitoreo y Retroalimentaci\u00f3n:<ul> <li>Principio: Implementar sistemas de monitoreo para obtener retroalimentaci\u00f3n r\u00e1pida sobre el rendimiento y la calidad del software en producci\u00f3n.</li> </ul> </li> <li>Responsabilidad Compartida:<ul> <li>Principio: Fomentar una mentalidad de responsabilidad compartida entre los equipos de desarrollo y operaciones en todo el ciclo de vida del software.</li> </ul> </li> </ol>"},{"location":"6_attachments/#como-se-utiliza-hoy-en-dia","title":"C\u00f3mo se utiliza hoy en d\u00eda","text":"<p>La cultura DevOps se implementa mediante la adopci\u00f3n de pr\u00e1cticas y herramientas espec\u00edficas, como:</p> <ul> <li>Despliegue Continuo (Continuous Deployment): Automatizaci\u00f3n del proceso de liberaci\u00f3n de software para que nuevas versiones puedan ser implementadas de manera r\u00e1pida y segura.</li> <li>Infraestructura como C\u00f3digo (Infrastructure as Code - IaC): Definir y gestionar la infraestructura de manera automatizada, trat\u00e1ndola como c\u00f3digo, lo que facilita la reproducibilidad y escalabilidad.</li> <li>Integraci\u00f3n Continua (Continuous Integration): Integrar el c\u00f3digo de los desarrolladores en un repositorio compartido varias veces al d\u00eda, lo que facilita la detecci\u00f3n temprana de errores.</li> <li>Monitoreo Continuo y An\u00e1lisis de Logs: Utilizar herramientas para monitorear el rendimiento en tiempo real y analizar los registros para identificar problemas y \u00e1reas de mejora.</li> </ul> <p>A modo de s\u00edntesis, la cultura DevOps busca mejorar la colaboraci\u00f3n, eficiencia y velocidad en el desarrollo y despliegue de software, y se apoya en la automatizaci\u00f3n, la entrega continua y la responsabilidad compartida entre los equipos de desarrollo y operaciones.</p>"},{"location":"6_attachments/#objetivos-smart","title":"Objetivos SMART","text":"<p>Los objetivos SMART son una metodolog\u00eda utilizada para establecer metas y objetivos de manera clara y espec\u00edfica. El acr\u00f3nimo SMART representa los criterios que deben cumplir estos objetivos:</p> <ol> <li>Espec\u00edficos (Specific): Los objetivos deben ser claros y espec\u00edficos, evitando ambig\u00fcedades y definiciones vagas.</li> <li>Medibles (Measurable): Deben incluir criterios cuantificables para evaluar el progreso y determinar cu\u00e1ndo se ha alcanzado el objetivo.</li> <li>Alcanzables (Achievable): Los objetivos deben ser realistas y alcanzables dentro del contexto y recursos disponibles.</li> <li>Relevantes (Relevant): Deben estar alineados con los objetivos generales y estrat\u00e9gicos de la organizaci\u00f3n.</li> <li>Temporizables (Time-bound): Deben tener un plazo o per\u00edodo de tiempo espec\u00edfico para su cumplimiento.</li> </ol> <p>Relacionando los objetivos SMART con Scrum en el contexto de Backlog, Sprint o Tarea:</p> <ul> <li>Backlog: Los elementos del Product Backlog en Scrum se benefician al ser definidos de manera SMART. Cada elemento debe ser espec\u00edfico en cuanto a su funcionalidad, medible en t\u00e9rminos de valor para el usuario, alcanzable dentro del alcance del proyecto, relevante para los objetivos del producto y con un plazo temporal definido.</li> <li>Sprint: Los objetivos de cada Sprint, establecidos durante la planificaci\u00f3n del Sprint, tambi\u00e9n pueden seguir la metodolog\u00eda SMART. Al ser espec\u00edficos, medibles, alcanzables, relevantes y temporizables, estos objetivos guiar\u00e1n el trabajo del equipo durante el Sprint.</li> <li>Tarea: Incluso a nivel de tareas individuales dentro de un Sprint, la aplicaci\u00f3n de la metodolog\u00eda SMART puede ser \u00fatil. Cada tarea puede ser definida de manera espec\u00edfica, medible en t\u00e9rminos de esfuerzo o resultados, alcanzable para un miembro del equipo, relevante para los objetivos del Sprint y con un plazo temporal asignado.</li> </ul> <p>La aplicaci\u00f3n de objetivos SMART en Scrum contribuye a una mayor claridad, enfoque y medici\u00f3n del progreso en el desarrollo de software, alineando los esfuerzos del equipo con metas claras y alcanzables.</p>"},{"location":"6_attachments/#a-modo-de-sintesis","title":"A modo de s\u00edntesis","text":"<p>Las metodolog\u00edas \u00e1giles, encabezadas por el Manifiesto \u00c1gil, han transformado la forma en que se aborda el desarrollo de software al promover valores como la flexibilidad, la colaboraci\u00f3n y la entrega continua de valor al cliente. SCRUM, una de las metodolog\u00edas \u00e1giles m\u00e1s populares, opera bajo los principios del Manifiesto \u00c1gil y estructura el desarrollo en sprints, con roles claramente definidos y un enfoque en la transparencia y adaptabilidad. La cultura DevOps, por otro lado, se alinea con los principios \u00e1giles al fomentar la colaboraci\u00f3n estrecha entre los equipos de desarrollo y operaciones, buscando la automatizaci\u00f3n y la entrega continua. En este contexto, los objetivos SMART se integran como una metodolog\u00eda clave para establecer metas claras, medibles y alcanzables, proporcionando un marco estructurado que puede aplicarse tanto a la gesti\u00f3n del backlog en SCRUM como a los objetivos espec\u00edficos de cada sprint. La combinaci\u00f3n de metodolog\u00edas \u00e1giles, SCRUM, la cultura DevOps y objetivos SMART crea un entorno de desarrollo flexible, colaborativo y orientado a resultados, permitiendo a los equipos adaptarse r\u00e1pidamente a los cambios, mejorar continuamente y cumplir con los objetivos estrat\u00e9gicos de la organizaci\u00f3n.</p>"},{"location":"6_attachments/#vagrant","title":"Vagrant","text":"<p>Vagrant es una herramienta que pod\u00e9s usar para crear y gestionar entornos de desarrollo virtualizados de manera f\u00e1cil y reproducible. Su uso t\u00edpico es facilitar la creaci\u00f3n de m\u00e1quinas virtuales con configuraciones espec\u00edficas para el desarrollo de proyectos.</p> <p>Para comenzar un proyecto de Vagrant en el directorio /vagrant, el usuario puede seguir estos pasos:</p> <ol> <li>Instalaci\u00f3n de Vagrant: Antes que nada, necesit\u00e1s instalar Vagrant en tu m\u00e1quina. Esto se puede hacer descargando el instalador desde el sitio oficial y siguiendo las instrucciones.</li> <li>Creaci\u00f3n de un archivo Vagrantfile: En el directorio donde quer\u00e9s iniciar el proyecto, cre\u00e1 un archivo llamado <code>Vagrantfile</code>. Este archivo contendr\u00e1 la configuraci\u00f3n de la m\u00e1quina virtual, como el sistema operativo, la cantidad de memoria RAM, etc.</li> <li>Configuraci\u00f3n del Vagrantfile: Dentro del Vagrantfile, especific\u00e1 la configuraci\u00f3n deseada. Por ejemplo, pod\u00e9s elegir un sistema operativo base, asignar recursos como CPU y RAM, y configurar la red.</li> <li>Inicializaci\u00f3n de la m\u00e1quina virtual: Ejecut\u00e1 el comando <code>vagrant up</code> en el directorio donde se encuentra el Vagrantfile. Este comando crear\u00e1 y provisionar\u00e1 la m\u00e1quina virtual seg\u00fan la configuraci\u00f3n especificada.</li> <li>Acceso a la m\u00e1quina virtual: Utiliz\u00e1 el comando <code>vagrant ssh</code> para acceder a la m\u00e1quina virtual reci\u00e9n creada. Esto abrir\u00e1 una conexi\u00f3n SSH a la m\u00e1quina virtual.</li> </ol> <p>Algunas ventajas clave de utilizar Vagrant para levantar m\u00faltiples entornos de desarrollo son:</p> <ul> <li>Reproducibilidad: Con Vagrant, se puede garantizar que todos los miembros del equipo tengan exactamente el mismo entorno de desarrollo, evitando problemas de compatibilidad.</li> <li>Portabilidad: Los entornos Vagrant son independientes de la m\u00e1quina host, lo que significa que pod\u00e9s compartir el mismo entorno de desarrollo en diferentes sistemas operativos.</li> <li>Aislamiento: Cada proyecto puede tener su propio entorno virtualizado, evitando conflictos entre dependencias y facilitando la gesti\u00f3n de versiones de software.</li> <li>Eficiencia en el uso de recursos: Vagrant permite ejecutar varias m\u00e1quinas virtuales de manera eficiente, lo que es \u00fatil para simular entornos complejos, como redes privadas virtuales (VPNs) o arquitecturas de microservicios.</li> </ul>"},{"location":"6_attachments/#ejemplo-de-una-vagrantfile","title":"Ejemplo de una Vagrantfile","text":"<p>Crearemos un entorno de 3 m\u00e1quinas, una master y 2 nodos a los cuales les aplicaremos configuraciones generales y particulares a cad auno. Importante aclarar que usaremos una imagen distinta a la vista antes, en este caso ser\u00e1 <code>ubuntu/trusty64</code>.</p> <p>\u00a1Importante! Para poder configurar cierta red privada deberemos crear o modificar el archivo <code>/etc/vbox/networks.conf</code> a\u00f1adiendo la red de la siguiente manera:</p> <pre><code>sudo su\necho \"* 0.0.0.0/0\" &gt; networks.conf\n</code></pre> <p>Primero vamos a crear las claves p\u00fablicas y privadas para las conexiones SSH:</p> <p>Creamos nuestra propia clave p\u00fablica y privada con <code>ssh-keygen</code>, procuramos no poner passphrase para que no se la solicite a las VMs a la hora de iniciarlas.</p> <pre><code># ~/.ssh/\n&gt; ssh-keygen -t rsa -b 4096\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/aagustin/.ssh/id_rsa): vagrant_key\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in vagrant_key\nYour public key has been saved in vagrant_key.pub\nThe key fingerprint is:\nSHA256:K4v2o7EKOLfjCMLk7zVD4v234234c06peueU aagustin@hp-agustin\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|                 |\n|                 |\n|                 |\n| . . . .S     .  |\n|= . 3 o ..   o.. |\n|*o.+.*.... +.++. |\n|o=o.B4==  . *++  |\n|..=*+*=++ .+o. E |\n+----[SHA256]-----+\n</code></pre> <p>Ahora si, escribimos el archivo Vagrantfile:</p> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\n\nVagrant.configure(\"2\") do |config|\n\n  # Image configuration (for all vm's)\n  config.vm.box = \"bento/ubuntu-22.04\"\n  config.vm.box_version = \"202309.08.0\"\n\n  # SSH (for all vm's)\n  config.ssh.insert_key = false\n  config.ssh.forward_agent = true  \n  config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]     \n  config.vm.provision \"file\", source: \"/home/aagustin/.ssh/vagrant_key.pub\", destination: \"/home/vagrant/.ssh/authorized_keys\"\n\n  # Declaring master node and defining it like a primary machine\n  config.vm.define \"master\", primary: true do |master|\n\n    # Resources (provider)\n    master.vm.provider \"virtualbox\" do |vb|\n      vb.gui = false\n      vb.name = \"trusty64-master\"\n      vb.memory = \"2048\"\n      vb.cpus = \"2\"\n    end\n\n    # Configure synced folder\n    #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/master/\", \"/home/vagrant/\"\n\n    # Network configuration\n    master.vm.network \"public_network\",\n      bridge:\"wlo1\",\n      ip: \"192.168.102.102\",\n      netmask: \"255.255.255.0\"\n\n    master.vm.network \"private_network\",\n      ip: \"192.168.55.2\",\n      netmask: \"255.255.255.0\",\n      auto_config: false\n\n    master.vm.network \"forwarded_port\",\n      guest: 80,\n      host: 31002\n\n    # SSH\n    master.ssh.host = \"127.0.0.2\"\n    master.vm.network \"forwarded_port\",\n      guest: 22,\n      host: 2222,\n      host_ip:\"0.0.0.0\",\n      id: \"ssh\",\n      auto_correct: true\n\n    # Provisioning message\n    master.vm.provision \"shell\",\n      inline: \"echo Hello master\"\n  end\n\n  # Declaring secondary nodes (iteratively)\n  (1..2).each do |i|\n    config.vm.define \"node-#{i}\" do |node|\n\n      # Resources (provider)\n      node.vm.provider \"virtualbox\" do |vb|\n        vb.gui = false\n        vb.name = \"trusty64-node-#{i}\"\n        vb.memory = \"2048\"\n        vb.cpus = \"2\"\n      end\n\n      # Configure synced folder\n      #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/node-#{i}/\", \"/home/vagrant/\"\n\n      # Network configuration\n      node.vm.network \"public_network\",\n        bridge:\"wlo1\",\n        ip: \"192.168.102.10#{2+i}\",\n        netmask: \"255.255.255.0\"\n\n      node.vm.network \"private_network\",\n        ip: \"192.168.55.#{2+i}\",\n        netmask: \"255.255.255.0\",\n        auto_config: false\n\n      node.vm.network \"forwarded_port\",\n        guest: 80,\n        host: 31002+i\n\n      # SSH\n      node.ssh.host = \"127.0.0.#{2+i}\"\n      node.vm.network \"forwarded_port\",\n        guest: 22,\n        host: 2222+i,\n        host_ip:\"0.0.0.0\",\n        id: \"ssh\",\n        auto_correct: true\n\n      # Provisioning message\n      node.vm.provision \"shell\", inline: \"echo Hello node-#{i}\"\n    end      \n\n  end\n\nend\n</code></pre> <p>Notar que en la l\u00ednea <code>config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]</code> ponemos dos opciones, lo que logramos con esto es que use primero la por default y luego de lograda la conexi\u00f3n ya configura la clave p\u00fablica, entonces podemos acceder con la clave propia la pr\u00f3xima vez. Adem\u00e1s es importante que est\u00e9 desactivada la generaci\u00f3n por defecto de nuevas claves as\u00ed se usa la gen\u00e9rica, por eso tenemo la l\u00ednea <code>config.ssh.insert_key = false</code>.</p> <p>Levantamos nuestra configuraci\u00f3n:</p> <p>Podemos hacer <code>vagrant up</code> y por ultimo veremos el estado de estas con:</p> <pre><code>$ vagrant status\nCurrent machine states:\n\nmaster                    running (virtualbox)\nnode-1                    running (virtualbox)\nnode-2                    running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <p>Nos conectamos con las VPCs:</p> <p>Podemos acceder con SSH mediante:</p> <pre><code>ssh -p [puerto-vpc] vagrant@[ip-vpc] -i [ubicacion-priv-key]\n</code></pre> <p>Una vez hecha la conexi\u00f3n SSH, podemos ver la configuraci\u00f3n de la red que le hemos establecido a dicha m\u00e1quina virtual:</p> <pre><code>vagrant@vagrant:~$ ip -brief -c a\nlo               UNKNOWN        127.0.0.1/8 ::1/128 \neth0             UP             10.0.2.15/24 metric 100 fe80::a00:27ff:fe3b:cf90/64 \neth1             UP             192.168.102.102/24 fe80::a00:27ff:fe3b:4c8d/64 \neth2             UP             192.168.55.2/24 fe80::a00:27ff:fef5:3997/64\n</code></pre> <p>Levantamos un servicio de prueba:</p> <p>Podemos checkear el correcto funcionamiento de la IP p\u00fablica y el port forwarding levantando un servicio con python en el puerto 80 de nuestra vPc:</p> <pre><code>sudo python3 -m http.server 80\n</code></pre> <p>Y luego, podemos acceder desde el navegador de la m\u00e1quina host o cualquier navegador de cualquier dispositivo que est\u00e9 conectado a la misma red local:</p> <p></p> <p></p>"},{"location":"6_attachments/#creacion-del-entorno-de-laboratorio","title":"Creaci\u00f3n del entorno de laboratorio","text":""},{"location":"6_attachments/#armado-de-la-infraestructura-de-prueba-virtualbox-ubuntu-server","title":"Armado de la infraestructura de prueba - VirtualBox + Ubuntu Server","text":"<p>Teniendo ya los tres nodos levantados con Vagrant, podremos probar aprovisionar software a los mismos utilizando Ansible. Antes deber\u00e9 asegurarme de tener lo siguiente:</p> <ol> <li>Deshabilito el port forwarding en la m\u00e1quina host para evitar paquetes duplicados.</li> <li>Me aseguro de tener instalado open-ssh</li> </ol>"},{"location":"6_attachments/#aprovisionar-con-ansible-instalacion-y-conexion-del-host-con-el-servidor","title":"Aprovisionar con Ansible - Instalaci\u00f3n y conexi\u00f3n del host con el servidor","text":"<ol> <li>Instalamos Ansible en Ubuntu de la m\u00e1quina host:</li> </ol> <pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo apt-add-repository ppa:ansible/ansible\nsudo apt update\nsudo apt install ansible\n</code></pre> <ol> <li>Nos dirigimos a la carpeta de Ansible en nuestra m\u00e1quina host:</li> </ol> <pre><code>cd /etc/ansible\n</code></pre> <ol> <li>Veremos listados los siguientes archivos y directorios:</li> </ol> <pre><code>ansible.cfg  hosts        roles/\n</code></pre> <p>Nos nos haremos una copia de hosts en formato .yaml en nuestra carpeta de trabajo:</p> <pre><code>sudo cp ./hosts ~/workdir/hosts.yaml\n</code></pre> <p>El archivo <code>hosts.yaml</code> es el inventario donde tendremos listados todos nuestros equipos que queremos controlar.</p> <ol> <li>Configuramos el inventario <code>hosts.yaml</code> para el ejemplo, agregando las siguientes lineas:</li> </ol> <pre><code># Example for PPS\nmycluster:\n  hosts:\n    master:\n      ansible_host: 127.0.0.2\n      ansible_port: 2222\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n    nodo1:\n      ansible_host: 127.0.0.3\n      ansible_port: 2223\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n    nodo2:\n      ansible_host: 127.0.0.4\n      ansible_port: 2224\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n</code></pre> <p>Lo anterior es equivalente a crear un grupo de equipos (en nuestro caso es uno solo) llamado \"mycluster\" y dentro de ese grupo definimos los hosts llamados master, nodo1 y nodo2. Adem\u00e1s agregamos un usuario de ssh y una ruta para la llave privada, comentados, que nos servir\u00e1n luego:</p> <pre><code>ansible -i hosts.yaml  all --list-hosts\n</code></pre> <p>Donde el <code>-i</code> nos sirve para indicar que queremos usar un archivo en particular de inventario, que en nuestro caso es <code>hosts.yaml</code> (importante que estemos posicionados en el directorio de ansible <code>/etc/ansible</code> o que indiquemos la ruta completa del archivo de inventario). Este comando nos devolver\u00e1 el siguiente mensaje:</p> <pre><code>$ ansible -i hosts.yaml all --list-hosts\n  hosts (3):\n    master\n    nodo1\n    nodo2\n</code></pre> <p>Ahora deberemos configurar SSH para que Ansible pueda conectarse a los nodos que manejamos, en nuestro caso, a nuestra m\u00e1quina virtual. Para configurar SSH y permitir conexiones SSH a sistemas remotos, debemos seguir estos pasos para agregar nuestra clave p\u00fablica SSH al archivo <code>authorized_keys</code> en cada sistema remoto. En nuestro caso, ya lo hemos hecho con Vagrant:</p> <ul> <li>Generar un par de claves SSH (si a\u00fan no lo hemos hecho): Si no tenemos un par de claves SSH (una p\u00fablica y una privada), ppodemos generarlas usando el comando <code>ssh-keygen</code> en la terminal del host. Si deseamos utilizar la configuraci\u00f3n predeterminada y sin contrase\u00f1a, simplemente presionamos Enter cuando se nos solicite una contrase\u00f1a. Aqu\u00ed tienes un ejemplo:</li> </ul> <pre><code>$ ssh-keygen\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/aagustin/.ssh/id_rsa): id_rsa_ansible\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in id_rsa_ansible\nYour public key has been saved in id_rsa_ansible.pub\n...\n</code></pre> <p>Esto generar\u00e1 un par de claves SSH en tu directorio de inicio (por defecto) en los archivos <code>id_rsa_asible</code> (clave privada) y <code>id_rsa_ansible.pub</code> (clave p\u00fablica).</p> <ul> <li>Copiar nuestra clave p\u00fablica al sistema remoto: Ahora, debes copiar nuestra clave p\u00fablica (<code>id_rsa.pub</code> por defecto) al sistema remoto. Podemos hacerlo manualmente o utilizando el comando <code>ssh-copy-id</code>. Por ejemplo usando <code>ssh-copy-id</code>:</li> </ul> <pre><code>ssh-copy-id usuario@nombre_del_sistema_remoto\n</code></pre> <p>Esto copiar\u00e1 nuestra clave p\u00fablica al sistema remoto y la agregar\u00e1 al archivo <code>~/.ssh/authorized_keys</code> en ese sistema. Debemos asegurarnos de reemplazar <code>usuario</code> con nuestro nombre de usuario y <code>nombre_del_sistema_remoto</code> con la direcci\u00f3n IP o el nombre de host del sistema remoto.</p> <pre><code>$ ssh-copy-id vagrant@127.0.0.2\nThe authenticity of host '127.0.0.2 (127.0.0.2)' can't be established.\nED25519 key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxx.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'vagrant@127.0.0.2'\"\nand check to make sure that only the key(s) you wanted were added.\n</code></pre> <ul> <li>Iniciar sesi\u00f3n en el sistema remoto con SSH: Ahora, podemos iniciar sesi\u00f3n en el sistema remoto usando SSH y se utilizar\u00e1 nuestra clave p\u00fablica para autenticarnos:</li> </ul> <pre><code>ssh usuario@nombre_del_sistema_remoto\n</code></pre> <p>Si hemos configurado todo correctamente, no deber\u00eda ser solicitado para ingresar una contrase\u00f1a. En su lugar, se utilizar\u00e1 tu clave privada local para la autenticaci\u00f3n.</p> <pre><code> Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-83-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Mon Oct  9 02:46:20 PM UTC 2023\n\n  System load:  0.0                Processes:             140\n  Usage of /:   12.4% of 30.34GB   Users logged in:       0\n  Memory usage: 11%                IPv4 address for eth0: 10.0.2.15\n  Swap usage:   0%                 IPv4 address for eth1: 192.168.5.240\n\n\nThis system is built by the Bento project by Chef Software\nMore information can be found at https://github.com/chef/bento\nLast login: Mon Oct  9 14:32:17 2023 from 10.0.2.2\n</code></pre> <ul> <li>Repetir el proceso para otros sistemas remotos: Debes repetir estos pasos para cada sistema remoto al que deseemos acceder con SSH. Copia tu clave p\u00fablica al archivo <code>authorized_keys</code> en cada uno de esos sistemas.</li> </ul> <p>Ahora podemos corroborar la conexi\u00f3n con los mismos con un modulo de ansible llamado <code>ping</code>, para ello necesitamos unos pasos previos as\u00ed evitamos el error con el mensaje \"Permission denied (publickey,password)\" que sugiere que Ansible intent\u00f3 usar autenticaci\u00f3n mediante clave p\u00fablica SSH, pero no pudo autenticarse con \u00e9xito.</p> <p>Comando PING:</p> <p>Es momento entonces de aplicar el comando de <code>ping</code> de la siguiente manera:</p> <pre><code>ansible -i hosts.yaml all -m ping\n</code></pre> <p>Donde el <code>-m</code> indica que vamos a usar un m\u00f3dulo de Ansible.</p> <p>Si todo est\u00e1 correcto deber\u00e1 devolvernos un ping exitoso a cada una de las IP's que configuramos previamente como el siguiente:</p> <pre><code>&gt; ansible -i hosts.yaml all -m ping\nnodo1 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nnodo2 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nmaster | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>Podremos empezar a ejecutar comandos con Ansible, por ejemplo, para ver qu\u00e9 tipo de SO y qu\u00e9 versi\u00f3n tenemos instalada nos vamos a valer del siguiente comando:</p> <pre><code>ansible -i hosts.yaml nodo1 -a \"cat /etc/os-release\"\n</code></pre> <p>Donde el <code>-a</code> indica que vamos a pasar argumentos al m\u00f3dulo.</p> <p>B\u00e1sicamente lo que hacemos es enviar ese comando al nodo1, lo que nos devolver\u00e1 la lectura del archivo os-release, el cual contiene la versi\u00f3n del sistema.</p> <pre><code>nodo1 | CHANGED | rc=0 &gt;&gt;\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n</code></pre>"},{"location":"6_attachments/#aprovisionar-con-ansible-creacion-de-un-playbook","title":"Aprovisionar con Ansible - Creaci\u00f3n de un playbook","text":"<p>Ahora veremos lo que es un playbook, con el cual, haremos lo mismo que hacemos con la consola de comandos pero expresado en un archivo de instrucciones. Podremos simplificar la estructura de la siguiente manera:</p> <p>PLAYBOOK &gt; PLAYS &gt; TASKS</p> <p>El playbook es un archivo en formato <code>.yml</code> o <code>.yaml</code> . El cual, en principio crearemos en la carpeta de de trabajo <code>~/workdir/ansible/</code> para hacer m\u00e1s corta la ruta a la hora de escribir en consola, pero podr\u00eda estar en cualquier lado y la usar\u00edamos llamando a la ruta completa.</p> <pre><code>&gt; ls\nhosts.yaml  playbook.yaml\n</code></pre> <p>Permisos de usuario root en caso de ser necesario (no es el nuestro):</p> <p>Antes de poder ejecutar cualquier tarea nos debemos asegurar de tener los permisos correspondientes, para ello  vamos a configurar sudo sin contrase\u00f1a para el usuario Ansible para permitir que el usuario Ansible (el usuario con el que nos conectaamos) ejecute comandos sin requerir una contrase\u00f1a. Esto se hace editando el archivo de configuraci\u00f3n sudo <code>(/etc/sudoers)</code> en el host remoto y agregando una entrada que permita comandos espec\u00edficos sin contrase\u00f1a.</p> <p>Agregar una entrada en <code>/etc/sudoers</code> para permitir que el usuario Ansible ejecute comandos como root sin contrase\u00f1a:</p> <ul> <li>Accedemos por SSH a la m\u00e1quina de destino:</li> </ul> <pre><code>ssh vagrant@127.0.0.2\n</code></pre> <ul> <li>Abrimos el archivo sudoers para edici\u00f3n utilizando un editor de texto en el host remoto (como visudo que garantiza que no se cometan errores de sintaxis) para abrir el archivo sudoers con privilegios de superusuario:</li> </ul> <pre><code>sudo visudo\n</code></pre> <ul> <li>Agregamos la entrada para el usuario de Ansible en el archivo sudoers para permitir al usuario de Ansible ejecutar comandos sin requerir una contrase\u00f1a. La entrada debe tener el siguiente formato:</li> </ul> <pre><code># Ansible root privileges\nvagrant ALL=(ALL:ALL) NOPASSWD: ALL\n</code></pre> <ul> <li>Guardamos y cerramos.</li> </ul> <p>Esto permite que el usuario Ansible ejecute cualquier comando como root sin requerir una contrase\u00f1a. Deberemos tener en cuenta que esta opci\u00f3n tiene implicaciones de seguridad y debe usarse con precauci\u00f3n.</p> <p>Playbook</p> <p>Continuando con el playbook, la estructura de este archivo para el ejemplo de la instalaci\u00f3n de la biblioteca <code>nano</code> deber\u00e1 ser la siguiente:</p> <pre><code>---\n- name: I want to install vim # Name of the play\nhosts: mycluster #  Name of the machine or a group of machines\nbecome: yes # Adding root privileges\nbecome_method: sudo # Uses sudo to get all privileges\nbecome_user: vagrant # Once you use sudo, you become root user\ntasks:\n- name: Install vagrant # Name of the task\napt: # Name of the module\nname: vim # Library to install\nstate: latest # ersion of that library\n</code></pre> <p>Para ejecutar ese archivo de instrucciones (<code>playbook.yml</code>), usamos el siguiente comando de Ansible:</p> <pre><code>ansible-playbook -i hosts.yaml playbook.yaml\n</code></pre> <p>Lo cual nos devolver\u00e1 lo siguiente:</p> <pre><code>&gt; ansible-playbook -i hosts.yaml playbook.yaml\n\nPLAY [I want to install vim] ****************************************************************************\n\nTASK [Gathering Facts] **********************************************************************************\nok: [master]\nok: [nodo2]\nok: [nodo1]\n\nTASK [Install vim] **************************************************************************************\nok: [master]\nok: [nodo2]\nok: [nodo1]\n\nPLAY RECAP **********************************************************************************************\nmaster                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nnodo1                      : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nnodo2                      : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Veremos que en nuestro caso, no hubo instalaci\u00f3n de nano porque ya existia entonces, se realizaron con \u00e9xito las tareas pero no hubo cambio alguno.</p> <p>Importante que a la hora de ejecutar dichos comandos, Ansible no har\u00e1 cambios si el estado deseado ya se ha logrado previamente. Por ejemplo, en este caso que queremos instalar nano suponiendo de que no estaba instalado, cuando lo ejecutemos por primera vez detectaremos que hay cambios, pero si lo ejecutamos por segunda vez veremos que habr\u00e1 cero cambios ya que el estado deseado de tener instalado nano ya se ha cumplido.</p> <p>Si el estado deseado de la tarea fuera \"absent\" en lugar de \"latest\", cuando lo corramos de nuevo, buscar\u00e1 que dicha librer\u00eda no est\u00e9, nuevamente habr\u00e1 un cambio y ser\u00e1 la eliminaci\u00f3n de dicha librer\u00eda.</p>"},{"location":"6_attachments/#aprovisionamiento-de-kubernetes","title":"Aprovisionamiento de Kubernetes","text":""},{"location":"6_attachments/#comparacion-de-diferentes-tecnologias","title":"Comparaci\u00f3n de diferentes tecnolog\u00edas","text":"<p>Deberemos explorar diferentes alternativas a la hora de ver qu\u00e9 nos conviene implementar, para ello se presentan las siguientes:</p> <p>Kubernetes (k8s): Kubernetes es la orquestaci\u00f3n de contenedores m\u00e1s robusta y ampliamente adoptada. Es ideal para despliegues a gran escala, gestionando cl\u00fasteres de contenedores con una arquitectura maestra-nodo. Ofrece una amplia gama de caracter\u00edsticas y es altamente personalizable, pero su complejidad puede ser abrumadora para proyectos m\u00e1s peque\u00f1os.</p> <p>K3s: K3s es una versi\u00f3n liviana de Kubernetes dise\u00f1ada para entornos con recursos limitados, como entornos de desarrollo local o dispositivos IoT. K3s simplifica la implementaci\u00f3n y gesti\u00f3n de Kubernetes al reducir el n\u00famero de componentes y requisitos del sistema. Esto lo hace m\u00e1s accesible para desarrolladores individuales y equipos que buscan una soluci\u00f3n m\u00e1s \u00e1gil.</p> <p>K0s: K0s es otra alternativa ligera a Kubernetes, pero se destaca por ser un cl\u00faster autocontenido y sin dependencias externas. Esto simplifica a\u00fan m\u00e1s la implementaci\u00f3n y permite ejecutar cl\u00fasteres de Kubernetes sin conexi\u00f3n a Internet. K0s es adecuado para escenarios donde la conectividad externa puede ser limitada o poco confiable.</p> <p>Minikube: Minikube es una herramienta que facilita la ejecuci\u00f3n de un cl\u00faster de Kubernetes de un solo nodo en entornos locales, como m\u00e1quinas de desarrollo. Aunque es menos adecuado para producciones a gran escala, es una opci\u00f3n pr\u00e1ctica para probar y desarrollar aplicaciones en un entorno controlado. Minikube permite a los desarrolladores experimentar con Kubernetes sin la necesidad de configuraciones complejas.</p> <p>Por el momento centraremos nuestra atenci\u00f3n en k8s y k0s, que a modo general podremos comparar rendimientos entre uno m\u00e1s completo y uno m\u00e1s simple, adem\u00e1s podremos comparar la facilidad de instalaci\u00f3n de los mismos.</p>"},{"location":"6_attachments/#k0s-version-alternativa-y-ligera-de-k8s","title":"k0s: Versi\u00f3n alternativa y ligera de K8s","text":"<p>K0s (pronunciado \"k-zeros\") es una plataforma Kubernetes ligera y autosuficiente dise\u00f1ada para ser f\u00e1cilmente desplegada en diferentes entornos, incluso aquellos con restricciones de conectividad. A diferencia de las implementaciones de Kubernetes convencionales, k0s es un cl\u00faster aut\u00f3nomo y no requiere de componentes externos para su funcionamiento. A continuaci\u00f3n, se detallan sus componentes y funcionalidades clave:</p> <ol> <li> <p>API Server (Servidor de API): El API Server de k0s es responsable de gestionar las operaciones y comunicaci\u00f3n en el cl\u00faster. Proporciona una interfaz para interactuar con los recursos de Kubernetes.</p> </li> <li> <p>Controller Manager (Administrador de Controladores): Este componente controla los controladores del sistema, que son procesos que regulan el estado del cl\u00faster y realizan acciones en respuesta a los cambios.</p> </li> <li> <p>Scheduler (Programador): El Scheduler se encarga de distribuir los pods en los nodos disponibles en funci\u00f3n de las capacidades y restricciones de estos.</p> </li> <li> <p>etcd: K0s utiliza una versi\u00f3n embebida de etcd como almacenamiento de datos distribuido para mantener la coherencia del estado del cl\u00faster.</p> </li> <li> <p>Kubelet y Kube-proxy: Estos componentes son esenciales en cualquier implementaci\u00f3n de Kubernetes. Kubelet es responsable de ejecutar contenedores en los nodos, mientras que Kube-proxy facilita la comunicaci\u00f3n de red entre los diferentes servicios.</p> </li> <li> <p>CoreDNS: Proporciona servicios de resoluci\u00f3n de nombres en el cl\u00faster, permitiendo la comunicaci\u00f3n entre los servicios por nombre en lugar de direcciones IP.</p> </li> <li> <p>Kubelet y Kube-proxy: Estos componentes son esenciales en cualquier implementaci\u00f3n de Kubernetes. Kubelet es responsable de ejecutar contenedores en los nodos, mientras que Kube-proxy facilita la comunicaci\u00f3n de red entre los diferentes servicios.</p> </li> </ol> <p>K0s es especialmente adecuado para entornos donde la conectividad a Internet es limitada o inestable, ya que puede operar de manera completamente aut\u00f3noma. Su dise\u00f1o ligero y su capacidad para funcionar como un cl\u00faster aut\u00f3nomo lo hacen apropiado para despliegues en dispositivos de borde (edge computing), entornos de desarrollo local y escenarios donde la simplicidad y la independencia de infraestructura externa son prioritarias. La facilidad de implementaci\u00f3n y la capacidad de operar en entornos variados hacen que k0s sea una opci\u00f3n atractiva para casos de uso diversos.</p> <p>Aprovisionamiento de k0s con Ansible sobre VMs de Vagrant</p> <p>Creamos la carpeta k0s, donde aplicaremos primero el siguiente comando:</p> <pre><code>vagrant init\n</code></pre> <p>Dentro de la misma carpeta deberemos tener la siguiente estructura de archivos:</p> <pre><code>k0s/\n|_ Vagrantfle\n|_ ansible/\n    |_ playbook.yaml\n    |_ inventory/\n    |    |_ inventory.yaml\n    |_ roles/\n        |_ .../\n        |_ .../\n            |_ .../\n</code></pre> <p>Ahora veremos que poner dentro de cada archivo.</p> <p>En la <code>Vagrantfile</code> deberemos tener lo siguiente:</p> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\n\nVagrant.configure(\"2\") do |config|\n\n  # Image configuration (for all vm's)\n  config.vm.box = \"bento/ubuntu-22.04\"\n  config.vm.box_version = \"202309.08.0\"\n\n  # SSH (for all vm's)\n  config.ssh.insert_key = false\n  config.ssh.forward_agent = true  \n  config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]     \n  config.vm.provision \"file\", source: \"/home/aagustin/.ssh/vagrant_key.pub\", destination: \"/home/vagrant/.ssh/authorized_keys\"\n\n  # How many VMs to create\n  VMS = 3\n\n  # We need at least:\n  # -initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes.\n  # -controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane.\n  # -worker = must contain at least one node so that we can deploy Kubernetes objects.\n\n\n  # Declaring nodes (iteratively)\n  (1..VMS).each do |i|\n    config.vm.define \"k0s-#{i}\" do |node|\n\n      # Resources (provider)\n      node.vm.provider \"virtualbox\" do |vb|\n        vb.gui = false\n        vb.name = \"trusty64-k0s-#{i}\"\n        vb.memory = \"1024\"\n        vb.cpus = \"2\"\n      end\n\n      # Configure synced folder\n      #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/k0s-#{i}/\", \"/home/vagrant/\"\n\n      # Network configuration\n      node.vm.network \"public_network\",\n        bridge:\"wlo1\",\n        ip: \"192.168.102.20#{1+i}\",\n        netmask: \"255.255.255.0\"\n\n      node.vm.network \"private_network\",\n        ip: \"192.168.55.#{1+i}\",\n        netmask: \"255.255.255.0\",\n        virtualbox__intnet: true\n        #auto_config: false\n\n      node.vm.network \"forwarded_port\",\n        guest: 80,\n        host: 31001+i\n\n      # SSH\n      node.ssh.host = \"127.0.0.#{1+i}\"\n      node.ssh.forward_agent = true\n      node.vm.network \"forwarded_port\",\n        guest: 22,\n        host: 2221+i,\n        host_ip:\"0.0.0.0\",\n        id: \"ssh\",\n        auto_correct: true\n\n      # Provisioning message\n      node.vm.provision \"shell\", inline: \"echo Hello node-#{i}\"\n    end      \n\n  end\n\nend\n</code></pre> <p>Ejecutaremos el comando <code>vagrant up</code> para tener las m\u00e1quinas virtuales. En este caso ya supusismos que tenemos las claves privadas y p\u00fablicas creadas por lo que no deber\u00edamos tener problema. Vemos que el <code>status</code> es el siguiente:</p> <pre><code>&gt; vagrant status\nCurrent machine states:\n\nk0s-1                     running (virtualbox)\nk0s-2                     running (virtualbox)\nk0s-3                     running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <p>Ahora podemos crear el inventario de Ansible <code>inventory.yaml</code>:</p> <pre><code>---\nall:\n  children:\n    initial_controller:\n      hosts:\n        k0s-1:\n    controller:\n      hosts:\n        k0s-2:\n    worker:\n      hosts:\n        k0s-3:\n  hosts:\n    k0s-1:\n      ansible_ssh_host: 127.0.0.2\n      ansible_ssh_port: 2222\n    k0s-2:\n      ansible_ssh_host: 127.0.0.3\n      ansible_ssh_port: 2223\n    k0s-3:\n      ansible_ssh_host: 127.0.0.4\n      ansible_ssh_port: 2224\n  vars:\n    ansible_user: vagrant\n    ansible_private_key: /home/aagustin/.ssh/vagrant_key\n</code></pre> <p>Con el inventario creado, podemos controlar que tenemos los hosts bien configurados y hacer un ping para ver si tenemos conectividad:</p> <ul> <li>Listamos todos los hosts:</li> </ul> <pre><code>&gt; ansible -i ansible/inventory/inventory2.yaml --list-hosts all\n  hosts (3):\n    k0s-1\n    k0s-2\n    k0s-3\n</code></pre> <ul> <li>Ejecutamos el comando PING:</li> </ul> <pre><code>&gt; ansible -i ansible/inventory/inventory.yaml -m ping all      \nk0s-1 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nk0s-3 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nk0s-2 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>Recordar que si es la primera vez que le damos up y no indicamos ningun fingerprint deberemos aceptar que queremos ingresar sin fingerprint poniendo <code>yes</code> en la terminal cuando nos pregunte.</p> <p>Ahora nos disponemos a crear el <code>playbook.yaml</code>, con el siguiente contenido:</p> <pre><code>---\n\n- hosts: initial_controller:controller:worker\n  name: Download k0s on all nodes\n  become: yes\n  roles:\n    - role: download\n      tags: download\n    - role: prereq\n      tags: prereq\n\n- hosts: initial_controller\n  gather_facts: yes\n  become: yes\n  name: Configure initial k0s control plane node\n  roles:\n    - role: k0s/initial_controller\n      tags: init\n\n- hosts: controller\n  gather_facts: yes\n  become: yes\n  serial: 1\n  name: Configure k0s control plane nodes\n  roles:\n    - role: k0s/controller\n      tags: server\n\n- hosts: worker\n  become: yes\n  name: Configure k0s worker nodes\n  roles:\n    - role: k0s/worker\n      tags: worker\n</code></pre> <p>Este se encargar\u00e1 de llamar a las diferentes plays que ejecutan ciertas tareas en los diferentes hosts. Por eso es necesario tener el contenido de la carpeta <code>roles/</code> y sus respectivos plays.</p> <p>Una vez tenemos esto, podemos ejecutar el aprovisionamiento con Asible a los nodos creados:</p> <pre><code>ansible-playbook ansible/playbook.yaml -i ansible/inventory/inventory.yaml\n</code></pre>"},{"location":"6_attachments/#k8s-kubernetes-convencional","title":"K8s: Kubernetes convencional","text":"<p>Kubernetes (k8s) es una plataforma de c\u00f3digo abierto dise\u00f1ada para automatizar, escalar y operar aplicaciones en contenedores. Su arquitectura se basa en un modelo maestro-nodo que coordina la gesti\u00f3n de contenedores en un cl\u00faster. Aqu\u00ed se describen sus principales componentes y funcionalidades:</p> <ol> <li> <p>API Server (Servidor de API): Act\u00faa como la interfaz principal para la gesti\u00f3n y control del cl\u00faster, permitiendo la interacci\u00f3n con los objetos de Kubernetes, como pods, servicios y vol\u00famenes.</p> </li> <li> <p>etcd: Es un almac\u00e9n de datos distribuido que mantiene la configuraci\u00f3n del cl\u00faster y el estado del mismo, garantizando la coherencia entre los nodos maestros.</p> </li> <li> <p>Control Plane (Plano de Control): Incluye componentes como el API Server, etcd, el Controller Manager y el Scheduler, trabajando en conjunto para tomar decisiones sobre el estado del cl\u00faster y coordinar las acciones necesarias.</p> </li> <li> <p>Kubelet: Este agente se ejecuta en cada nodo del cl\u00faster y es responsable de asegurar que los contenedores est\u00e9n en ejecuci\u00f3n. Interact\u00faa con el API Server para recibir y ejecutar las instrucciones.</p> </li> <li> <p>Kube-proxy: Facilita la comunicaci\u00f3n de red entre los pods y gestiona las reglas de red, como el enrutamiento y el balanceo de carga.</p> </li> <li> <p>Pods: La unidad m\u00e1s peque\u00f1a en Kubernetes, que puede contener uno o m\u00e1s contenedores. Los pods comparten un espacio de red y almacenamiento, lo que facilita la comunicaci\u00f3n entre ellos.</p> </li> <li> <p>Services (Servicios): Proporcionan una abstracci\u00f3n para la comunicaci\u00f3n entre los diferentes pods, permitiendo la escalabilidad y la resiliencia de las aplicaciones.</p> </li> </ol> <p>Kubernetes es altamente vers\u00e1til y puede desplegarse en una variedad de entornos, desde infraestructuras locales hasta nubes p\u00fablicas. Es especialmente eficaz en entornos de producci\u00f3n donde la orquestaci\u00f3n y escalabilidad de contenedores son fundamentales. Kubernetes tambi\u00e9n es utilizado com\u00fanmente en entornos de desarrollo y pruebas para garantizar la coherencia entre los diferentes ciclos de vida de las aplicaciones. Su capacidad para gestionar cargas de trabajo en diversos entornos y su gran comunidad de usuarios lo hacen adecuado para una amplia gama de casos de uso.</p>"},{"location":"6_attachments/#automatizacion-de-la-implementacion-de-kubernetes-k8s-en-maquinas-vagrant-utilizando-ansible","title":"Automatizaci\u00f3n de la implementaci\u00f3n de Kubernetes (k8s) en m\u00e1quinas Vagrant utilizando Ansible","text":""},{"location":"6_attachments/#pasos-para-el-aprovisionamiento","title":"Pasos para el aprovisionamiento","text":"<ol> <li> <p>Cargar nuestra propia configuraci\u00f3n de Vagrant en <code>config_vms.yaml</code></p> </li> <li> <p>Detalles a tener en cuenta:</p> <ul> <li>Dependiendo de la cantidad de m\u00e1quinas virtuales que requieras para tu laboratorio deber\u00e1s cambiar la variable <code>vms</code> al n\u00famero correspondiente.</li> <li>Observar que la variable de <code>vb_memory</code> es igual a <code>2048</code> , sino habr\u00e1 problemas con la ejecuci\u00f3n de cualquier configuraci\u00f3n de Kubernetes debido a que el requerimiento minimo de memoria son 2GB.</li> <li>Recordar que debes cambiar las variables <code>pub_key_path</code> y <code>priv_key_path</code> con los valores correspondientes a la ruta hacia tus claves p\u00fablica y privada respectivamente.</li> <li>Recordar que debes cambiar la variable <code>base_pub_ip</code> a la correspondiente a tu red LAN del laboratorio u hogar.</li> <li>Recordar que debes cambiar la variable <code>bridged_iface</code> por la interfaz correspondiente a la que est\u00e1 conectada a tu red LAN del laboratorio u hogar.</li> </ul> <p>Deber\u00e1s colocar en la variable <code>env</code> la correspondiente a tu configuraci\u00f3n, quedando como sigue:</p> <pre><code>---\nvagrant_config:\nenv: 'tu_usuario'\nusers:\n    tu_usuario:\n      base_name: \"k8s\"\n      base_image: \"bento/ubuntu-22.04\"\n      base_image_version: \"202309.08.0\"\n      #### -&gt; Resto de tus configuraciones\n</code></pre> </li> </ol> <p>Ahora s\u00ed, podemos levantar las m\u00e1quinas virtuales estando en el directorio correspondiente al archivo <code>Vagrantfile</code>:</p> <pre><code>vagrant up\n</code></pre> <p>Para la eliminaci\u00f3n de las m\u00e1quianas creadas usar:</p> <pre><code>vagrant destroy\n</code></pre> <ol> <li>Entender el directory layout de <code>ansible/</code>:</li> </ol> <p>Existen \"buenas pr\u00e1cticas\" a la hora de acomodar los archivos que utilizamos para aprovisionar con Ansible. Pod\u00e9s encontrar m\u00e1s informaci\u00f3n ac\u00e1: Ansible Best Practicas - Directory Layout</p> <p>Luego de seguir a \u00e9ste se acomodaron los directorios y archivos de la siguiente manera:</p> <pre><code>&gt; tree\n.\n\u251c\u2500\u2500 cluster_reset.yml\n\u251c\u2500\u2500 cluster_setup.yml\n\u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 all.yml\n\u251c\u2500\u2500 host_vars\n\u251c\u2500\u2500 inventory.yml\n\u251c\u2500\u2500 old_files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-get-join-command.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-hosts.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-init-cluster.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-install-kubernetes-dependencies.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-join-workers.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ansible-vars.yml\n\u251c\u2500\u2500 reset.yml\n\u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 get_join_command\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 init_cluster\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 install_kubernetes_dependencies\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 join_workers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reset\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2514\u2500\u2500 site.yml\n</code></pre> <p>Cada uno de estos directorios y archivos juega un papel importante en la organizaci\u00f3n de las configuraciones y tareas de Ansible. Los elementos principales de la estructura de directorios son:</p> <ul> <li> <p><code>cluster_reset.yml</code> y <code>cluster_setup.yml</code>: Son los puntos de entrada para los playbooks de Ansible. Estos archivos especifican las tareas que se deben realizar en el entorno objetivo.</p> </li> <li> <p><code>group_vars/all.yml</code>: Ac\u00e1 se definen variables espec\u00edficas de grupo que se aplicar\u00e1n a todos los hosts en el inventario.</p> </li> <li> <p><code>host_vars/</code>: Este directorio se utilizar\u00eda para almacenar variables espec\u00edficas de host si es necesario.</p> </li> <li> <p><code>inventory.yml</code>: Este archivo es donde se define nuestro inventario, es decir, la lista de hosts en los que Ansible ejecutar\u00e1 las tareas. Puedes definir grupos de hosts y asignar variables en este archivo.</p> </li> <li> <p><code>old_files/</code>: Este directorio contiene archivos antiguos o archivos de configuraci\u00f3n anteriores que ya no se utilizan en el proyecto, es decir, los originales previos a la reestructuraci\u00f3n.</p> </li> <li> <p><code>reset.yml</code>: Un playbook para restablecer la configuraci\u00f3n de tu sistema o cl\u00faster.</p> </li> <li> <p><code>roles/</code>: Este directorio contiene los roles de Ansible, que son m\u00f3dulos reutilizables que definen tareas espec\u00edficas. Cada rol tiene subdirectorios para las tareas y las variables por defecto asociadas a ese rol.</p> <ul> <li> <p><code>get_join_command/</code>, <code>init_cluster/</code>, <code>install_kubernetes_dependencies/</code>, <code>join_workers/</code>, y <code>reset/</code> son los nombres de los roles que est\u00e1s utilizando en tu proyecto.</p> </li> <li> <p>Dentro de cada rol, hay subdirectorios <code>defaults</code> y <code>tasks</code> que contienen variables por defecto y tareas espec\u00edficas para ese rol.</p> </li> </ul> </li> <li> <p><code>site.yml</code>: Es un archivo de nivel superior que suele utilizarse para orquestar la ejecuci\u00f3n de varios playbooks y roles en un orden espec\u00edfico.</p> </li> </ul> <p>Notar\u00e1s que como extra tenemos el playbook y role para hacer un reset de las configuraciones, entonces no ser\u00e1 necesario eliminar todas las m\u00e1quinas virtuales para volver a comenzar en caso de error.</p> <ol> <li> <p>Comprender lo que hace cada role en <code>roles</code>:</p> </li> <li> <p><code>install_kubernetes_dependencies</code>:</p> <ul> <li>Instala paquetes necesarios para configurar Kubernetes y Docker, como <code>apt-transport-https</code>, <code>docker-ce</code>, <code>kubelet</code>, etc.</li> <li>Configura claves de firma y repositorios para Docker y Kubernetes.</li> <li>Asegura que Docker est\u00e9 habilitado y en funcionamiento.</li> <li>Deshabilita el archivo de swap y elimina las configuraciones de swap.</li> <li>Reinicia el sistema para aplicar los cambios.</li> </ul> </li> <li> <p><code>init_cluster</code>:</p> <ul> <li>Configura Docker para usar el controlador de cgroups systemd.</li> <li>Inicializa el cl\u00faster de Kubernetes con un comando <code>kubeadm init</code>, especificando una m\u00e1scara de subred para la red de pod.</li> <li>Crea un directorio <code>.kube</code> en el directorio de inicio del usuario.</li> <li>Configura el archivo de configuraci\u00f3n de Kubernetes en el directorio de inicio del usuario.</li> <li>Reinicia el servicio kubelet para aplicar las configuraciones.</li> <li>Descarga y aplica las configuraciones de red Calico y el panel de control de Kubernetes Dashboard.</li> </ul> </li> <li> <p><code>get_join_command</code>:</p> <ul> <li>Extrae el comando para unirse al cl\u00faster Kubernetes con el comando <code>kubeadm token create --print-join-command</code>.</li> <li>Guarda el comando de uni\u00f3n en un archivo local (<code>join_command.out</code>).</li> </ul> </li> <li> <p><code>join_workers</code>:</p> <ul> <li>Configura Docker para usar el controlador de cgroups systemd.</li> <li>Lee el comando de uni\u00f3n del archivo local.</li> <li>Ejecuta el comando de uni\u00f3n para agregar nodos trabajadores al cl\u00faster.</li> </ul> </li> <li> <p><code>reset</code>:</p> <ul> <li>Elimina los paquetes de Kubernetes y Docker instalados previamente.</li> <li>Elimina las claves de firma y los repositorios relacionados con Docker y Kubernetes.</li> <li>Elimina cualquier configuraci\u00f3n de intercambio y habilita el intercambio si estaba deshabilitado previamente.</li> <li>Elimina la configuraci\u00f3n del controlador de cgroups Docker.</li> <li>Reinicia el sistema para aplicar los cambios.</li> </ul> </li> <li> <p>Modificar la variable <code>ansible_private_key: tu_ruta/tu_clave_privada</code> del archivo <code>group_vars/all.yml</code>.</p> </li> <li> <p>Checkear que se hayan levantado correctamente las m\u00e1quinas virtuales:</p> </li> </ol> <pre><code>&gt; vagrant status\nCurrent machine states:\n\nk8s-1                     running (virtualbox)\nk8s-2                     running (virtualbox)\nk8s-3                     running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <ol> <li>\u00a1Atenci\u00f3n! Seg\u00fan tus requerimientos en cuanto a cantidad de m\u00e1quinas virtuales (cual n\u00famero definiste en la variable <code>vms</code> del archivo de configuraci\u00f3n <code>config_vms.yaml</code>) deber\u00e1s modificar el inventario:</li> </ol> <pre><code>---\n\nall:\nchildren:\n   kube_server:\n      hosts:\n      k8s-1:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 1 }}\"\n   kube_agents:\n      hosts:\n      k8s-2:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 2 }}\"\n      k8s-3:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 3 }}\"\n</code></pre> <p>En nuestro caso, tenemos 3 m\u00e1quinas virtuales, en el caso de haber m\u00e1s o menos nos aseguraremos de agregarla o eliminarla seg\u00fan corresponda.</p> <ol> <li>Comprobamos conectividad con todos los nodos:</li> </ol> <pre><code># En el directorio /ansible\nansible -i inventory.yml -m ping all\n</code></pre> <p>Obtendr\u00edamos el siguiente output:</p> <pre><code>k8s-1 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\nk8s-2 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\nk8s-3 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\n</code></pre> <ol> <li>Ejecutar el aprovisionamiento con Ansible:</li> </ol> <pre><code># En el directorio /ansible\nansible-playbook -vvv site.yml -i inventory.yml\n</code></pre> <p>En caso de querer resetear la configuraci\u00f3n:</p> <pre><code># 1) Ejecutamos el role\nansible-playbook reset.yml -i inventory.yml\n\n# 2) Eliminamos el archivo creado con los commands\nrm  rm join_command.out \n</code></pre> <p>En caso de querer ejecutar s\u00f3lo un role espec\u00edfico</p> <pre><code>ansible-playbook -vvv site.yml -i inventory.yml --tags tag_del_rol_a_ejecutar\n</code></pre> <ol> <li>Checkear que todo se haya ejecutado correctamente:</li> </ol> <p>Primero, veremos que el output luego de ejecutar el playbook de Ansible es el siguiente:</p> <pre><code>PLAY RECAP **********************************************************************************************\nk8s-1                      : ok=31   changed=21   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nk8s-2                      : ok=21   changed=12   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nk8s-3                      : ok=21   changed=16   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Segundo, ingresamos por SSH a la m\u00e1quina que hayamos definido como control-plane o master y nos fijaremos los nodos:</p> <pre><code>&gt; ssh -i ~/.ssh/vagrant_key vagrant@192.168.55.51\nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-83-generic x86_64)\n\n* Documentation:  https://help.ubuntu.com\n* Management:     https://landscape.canonical.com\n* Support:        https://ubuntu.com/advantage\n\nSystem information as of Thu Nov  2 05:33:07 PM UTC 2023\n\nSystem load:  1.34521484375      Users logged in:          0\nUsage of /:   15.6% of 30.34GB   IPv4 address for docker0: 172.17.0.1\nMemory usage: 12%                IPv4 address for eth0:    10.0.2.15\nSwap usage:   0%                 IPv4 address for eth1:    192.168.102.51\nProcesses:    166                IPv4 address for eth2:    192.168.55.51\n\n\nThis system is built by the Bento project by Chef Software\nMore information can be found at https://github.com/chef/bento\nLast login: Thu Nov  2 17:34:59 2023 from 192.168.55.1\n\nvagrant@k8s-1:~$ kubectl get nodes\nNAME    STATUS   ROLES                  AGE   VERSION\nk8s-1   Ready    control-plane,master   2m    v1.23.6\nk8s-2   Ready    &lt;none&gt;                 96s   v1.23.6\nk8s-3   Ready    &lt;none&gt;                 96s   v1.23.6\n</code></pre> <p>\u00a1Listo! Tenemos nuestro peque\u00f1o cluster de Kubernetes levantado en nuestro entorno de laboratorio.</p>"},{"location":"6_attachments/#extras","title":"Extras","text":""},{"location":"6_attachments/#test-mermaid","title":"Test Mermaid","text":"<pre><code>graph TD\n\nsubgraph Public Subnet\n  pub[\"Red P\u00fablica&lt;br&gt;IP de red 192.168.102.0/24&lt;br&gt;interface bridge wlo1\"]\nend\n\nsubgraph Control Plane - Master\n  c1((\"k8s-1&lt;br&gt;IP de host pub: 192.168.102.51&lt;br&gt;IP privada: 192.168.55.51\"))\nend\n\nsubgraph Workers\n  w1((\"k8s-3&lt;br&gt;IP de host pub: 192.168.102.52&lt;br&gt;IP privada: 192.168.55.52\"))\nend\n\nsubgraph Workers\n  w2((\"k8s-3&lt;br&gt;IP de host pub: 192.168.102.53&lt;br&gt;IP privada: 192.168.55.53\"))\nend\n\nsubgraph Private Subnet\n  priv[\"Red Privada&lt;br&gt;IP de red 192.168.55.0/24\"]\nend\n\nsubgraph Internet\n  internet((Internet))\nend\n\ninternet --- pub\npub --- c1\npub --- w1\npub --- w2\n\nc1 --- priv\nw1 --- priv\nw2 --- priv</code></pre>"}]}